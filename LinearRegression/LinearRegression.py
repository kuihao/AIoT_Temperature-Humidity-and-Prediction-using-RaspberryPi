'''
é¡Œç›®ï¼šæ¡ƒåœ’å¹³é® PM 2.5 é æ¸¬
'''
import sys
import numpy as np 
import matplotlib.pyplot as plt #ç•«å‡ºåœ–å‹ 
import pandas as pd #è³‡æ–™è™•ç†
pd.set_option("display.max_rows", 1000)    #è¨­å®šæœ€å¤§èƒ½é¡¯ç¤º1000rows
pd.set_option("display.max_columns", 1000) #è¨­å®šæœ€å¤§èƒ½é¡¯ç¤º1000columns
'''
[Load Train Data(åŒ¯å…¥è¨“ç·´è³‡æ–™)]
TrainData_2019_PingZhen.csv çš„è³‡æ–™ç‚º 12 å€‹æœˆä¸­ï¼Œæ¯å€‹æœˆå– 20 å¤©ï¼Œæ¯å¤© 24 å°æ™‚çš„è³‡æ–™(æ¯å°æ™‚è³‡æ–™æœ‰ 18 å€‹ features)
'''
data = pd.read_csv(r'LinearRegression\TrainingData_2019_Pinzhen\TrainData_2019_PingZhen.csv', encoding = 'utf-8')
'''
[Preprocessing(è³‡æ–™é è™•ç†)]
Column Name 'RAINFALL' çš„ NR(No Rain) å…¨éƒ¨æ”¹æˆ 0 ......[å·²å®Œæˆ]
'''
data = data.iloc[:, 3:]
# data[data == 'NR'] = 0
raw_data = data.to_numpy()
'''
[Extract Features (1)] ä»¥ 12 å€‹æœˆçš„ 15 å€‹ Features (å…±180é …) ç‚ºåˆ—ï¼Œå°æ™‚ç‚ºæ¬„
å°‡åŸå§‹æ•¸æ“š 3600 (ç­†) * 24 (hours) ä¾ç…§æœˆä»½é‡æ–°åˆ†çµ„æˆ 12 å€‹ 15 (features) * 480 (hours) çš„è³‡æ–™
'''
month_data = {}
for month in range(12):
    sample = np.empty([15, 480])
    for day in range(20):
        # sampleæ˜¯å®¹å™¨ï¼Œ15 featuresç‚ºåˆ—ï¼Œä¸€æ—¥24å°æ™‚ç‚ºä¸€æ¬„ï¼Œè¦è£20å¤©ï¼›row_dataæ¯æ¬¡é¸15ç­†feature(å«24æ¬„)ï¼Œä¸€å€‹æœˆæœ‰20å¤©çš„è³‡æ–™*ç›®å‰ç¬¬å¹¾å€‹æœˆ+å·²ç®—åˆ°ç¬¬å¹¾å¤©
        sample[:, day * 24 : (day + 1) * 24] = raw_data[15 * (20 * month + day) : 15 * (20 * month + day + 1), :]
    month_data[month] = sample
'''
[Extract Features (2)]æŠ½å‡ºè¨“ç·´è³‡æ–™
1. æ¯å€‹æœˆæœ‰ 15 (features) * 480 (hours) çš„æ•¸æ“šã€‚
2. å¾ä¸­ã€Œç”¢ç”Ÿè¨“ç·´è³‡æ–™ã€çš„è¦å‰‡æ˜¯ï¼š
   æ¯ 10 å€‹å°æ™‚çš„è³‡æ–™çµ„æˆä¸€ç­†è¨“ç·´è³‡æ–™ (æ„æ€æ˜¯åˆå¤œ12é»è‡³æ—©ä¸Š10é»ç‚ºç¬¬ä¸€ç­†ï¼Œå‡Œæ™¨1é»è‡³æ—©ä¸Š11é»ç‚ºç¬¬äºŒç­†ï¼Œä¾æ­¤é¡æ¨)ï¼Œ
   å…¶ä¸­ï¼Œå‰ 9 å°æ™‚çš„è³‡æ–™ä½œç‚º Input Datasï¼Œæœ€å¾Œ 1 å°æ™‚çš„ PM 2.5 æ•¸å€¼ä½œç‚º Output Dataï¼Œ
   å› æ­¤ï¼ŒInput Datas ç‚º 15 (features) * 9 (hours)ï¼ŒOutput Data ç‚º 1 (features) * 1 (hours)ã€‚
3. æ¯å€‹æœˆæ‰£é™¤æœ€å¾Œ9å°æ™‚è³‡æ–™ä¸æ‹¿ä¾†é æ¸¬(ç‚ºä½•ä¸æ˜¯ä¸€å¹´ç‚ºè¨ˆç®—ï¼Ÿ)ï¼Œå‰‡å¯é æ¸¬è³‡æ–™é‡æ˜¯ 480 - 9 = 471 å€‹å°æ™‚çš„ PM 2.5ï¼Œ
   ä¹Ÿå°±æ˜¯æ¯å€‹æœˆå¯å¾ä¸­æå– 471 ç­†è¨“ç·´è³‡æ–™ï¼Œæ›ç®—æˆæ•´å¹´å°±æ˜¯ 12 * 471 = 8,892 ç­†è¨“ç·´è³‡æ–™ã€‚
'''
# x ç‚ºè¨“ç·´è³‡æ–™çš„è¼¸å…¥ï¼Œåˆ—æ˜¯è¨“ç·´è³‡æ–™å€‹æ•¸(ä¸€å€‹æœˆ471ç­†ï¼Œä¹˜ä¸Š12å€‹æœˆ)ï¼Œæ¬„æ˜¯9å€‹å°æ™‚çš„Featureså¾ªç’°
x = np.empty([12 * 471, 15 * 9], dtype = float)
# y ç‚ºè¨“ç·´è³‡æ–™çš„è¼¸å‡ºï¼Œåˆ—æ˜¯è¨“ç·´è³‡æ–™å€‹æ•¸(ä¸€å€‹æœˆ471ç­†ï¼Œä¹˜ä¸Š12å€‹æœˆ)ï¼Œæ¬„æ˜¯ä¸‹ä¸€å°æ™‚çš„PM 2.5
y = np.empty([12 * 471, 1], dtype = float)
for month in range(12):
    for day in range(20):
        for hour in range(24):
            # ç”±æ–¼æ²’æœ‰éš”å¹´å¹´åˆçš„è³‡æ–™ï¼Œå› æ­¤å¹´åº•å‰©é¤˜çš„9å€‹å°æ™‚çš†ç„¡æ³•æ§‹æˆè¨“ç·´è³‡æ–™
            if day == 19 and hour > 14:
                continue
            # æ¯ä¸€ç­†è¨“ç·´è³‡æ–™æ˜¯æ–°å¢åˆ—(é‰›ç›´è»¸æ–°å¢)
            # x æ˜¯è¼¸å…¥ç«¯ï¼Œå– month_data [æœˆ][ 15å€‹featureså…¨è¦, éå¢çš„9å€‹å°æ™‚(æ¬„çš„ç¸½é•·åº¦å–®ä½æ˜¯20å¤©æ›ç®—å°æ™‚è¨ˆ)]
            # ç¶“éreshapeï¼Œnumphyå¯ä»¥å»£æ’­è‡³xçŸ©é™£ï¼Œä¸”åŒé¡çš„featureæœƒæ¥çºŒæ’åˆ—
            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)
            # y æ˜¯è¼¸å‡ºç«¯ï¼Œå– month_data [æœˆ][ PM 2.5çš„é …, ç¬¬10å€‹å°æ™‚(indexèµ·æ–¼0ï¼Œæ•…ç¬¬10å°æ™‚ä»¥9èµ·)]
            y[month * 471 + day * 24 + hour, 0] = month_data[month][7, day * 24 + hour + 9] #value
# print(x)
# print(y)

'''
shuffle å°‡è³‡æ–™æ‰“æ•£éš¨æ©Ÿæ’åº
ç›®çš„ï¼šä½¿å¾ŒçºŒåˆ‡å‰² train_setã€validation_set è³‡æ–™å¯ä»¥è¼ƒç‚ºå¹³å‡
'''
# box = np.concatenate((x, y), axis=1)
# box = pd.DataFrame(box)
# #print(box[0][0])
# box = box.sample(frac=1).reset_index(drop=True) # n=len(box.index)
# #print(box[0][0])
# x = np.array(box.iloc[:,0:135])
# y = np.array(box.iloc[:,135:])
# np.save(r'LinearRegression\TrainingData_2019_Pinzhen\x_tran_shuffle.npy', x)
# np.save(r'LinearRegression\TrainingData_2019_Pinzhen\y_tran_shuffle.npy', y)
'''
[Normalize (1)]
æ­£æ­¸åŒ–ï¼šç•¶è³‡æ–™ä¸æ˜¯æ•¸å€¼ï¼Œæˆ–æ˜¯é›–ç‚ºæ•¸å€¼å»æ˜¯é¡åˆ¥è³‡æ–™(å±¬æ€§è³‡æ–™ã€åç¾©å°ºåº¦)ï¼Œå‰‡éœ€ç¶“éç·¨ç¢¼è™•ç†
       (æœ¬å°ˆé¡Œå› ç‚ºä¸ç”¨ä½œæ•˜è¿°ã€æ¨è«–çµ±è¨ˆåˆ†æï¼Œå› æ­¤ä¸éœ€è¦)
æ¨™æº–åŒ–ï¼šæ¸¬é‡ä¸€çµ„æ•¸å€¼çš„é›¢æ•£ç¨‹åº¦ä½¿ç”¨ mean(x) å¹³å‡å€¼ã€Standard Deviation æ¨™æº–å·®
    æ–¹æ³•1 Max-Min å¸¸è¦‹çš„è³‡æ–™æ¨™æº–åŒ–æ–¹æ³•ï¼Œç°¡å–®ä¾†èªªï¼Œå°‡åŸå§‹è³‡æ–™çš„æœ€å¤§ã€æœ€å°å€¼mappingè‡³å€é–“[0,1]
          å…¬å¼ï¼šx_normalized = x - min(x) / max(x) - min(x)
    æ–¹æ³•2 æ¡ç”¨Z-Score (Zåˆ†æ•¸) åˆ©ç”¨åŸå§‹è³‡æ–™çš„å‡å€¼ï¼ˆmeanï¼‰å’Œæ¨™æº–å·®ï¼ˆstandard deviationï¼‰é€²è¡Œ
          è³‡æ–™çš„æ¨™æº–åŒ–ï¼Œé©ç”¨æ–¼è³‡æ–™çš„æœ€å¤§å€¼å’Œæœ€å°å€¼æœªçŸ¥çš„æƒ…æ³ï¼Œæˆ–æœ‰è¶…å‡ºå–å€¼ç¯„åœçš„é›¢ç¾¤è³‡æ–™çš„æƒ…æ³ã€‚
          å…¬å¼ï¼šæ–°è³‡æ–™ =ï¼ˆåŸå§‹è³‡æ–™-å‡å€¼ï¼‰/ æ¨™æº–å·®
'''
x = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\x_tran_shuffle.npy')
y = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\y_tran_shuffle.npy')
mean_x = np.mean(x, axis = 0) #15 * 9: axis = 0 æ˜¯å–é‰›ç›´æ–¹å‘ï¼Œå°12 * 471çš„è³‡æ–™ç®— Meanï¼Œå› æ­¤æœƒæœ‰ 15 * 9 å€‹Meanå€¼ 
std_x = np.std(x, axis = 0) #15 * 9 
for i in range(len(x)): #12 * 471
    for j in range(len(x[0])): #15 * 9 
        if std_x[j] != 0:
            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]
            # ç›®å‰æ˜¯å°‡ 15 * 9 å€‹ Featureséƒ½è¦–ç‚ºç¨ç«‹ä¸åŒç¨®é¡çš„ Feature
            # z-score (åˆç¨±Standard Score) çš„æ¨™æº–åŒ–æ–¹æ³•å°±æ˜¯å°‡å€‹åˆ¥ Feature éƒ½å¾ç›®å‰çš„è³‡æ–™é›†è£¡é¢ï¼Œè¨ˆç®—è‡ªå·±é€™ç¨® Feature çš„
            # å¹³å‡æ•¸åŠæ¨™æº–å·®ï¼Œå…¬å¼ã€Œé›¢å‡å·®èˆ‡æ¨™æº–å·®çš„æ¯”å€¼ã€å°±æ˜¯æ¯çµ„è³‡æ–™èˆ‡å¹³å‡æ•¸çš„æ¨™æº–åŒ–å¾Œã€Œç›¸å°è·é›¢ã€
            # ä¾ç…§ä¸­å¤®æ¥µé™å®šç†ï¼Œä¸–ç•Œä¸Šæ‰€æœ‰è³‡æ–™çš„åˆ†å¸ƒ99.9936%éƒ½æœƒåŒ…åœ¨å››å€‹æ¨™æº–å·®ä¹‹å…§ï¼Œå› æ­¤
            # z-score å…¬å¼å¯ä»¥é”åˆ°ä¸åŒç¨®é¡è³‡æ–™çµ±ä¸€æ¨™æº–åŒ–å‘ˆç¾çš„æ•ˆæœ
x
# print(x)
'''
[Split Training Data]
train_setç”¨ä¾†è¨“ç·´ï¼Œvalidation_setä¸æœƒè¢«æ”¾å…¥"train_set" and "validation_set"è¨“ç·´ã€åªæ˜¯ç”¨ä¾†é©—è­‰
'''
import math
x_train_set = x[: math.floor(len(x) * 0.8), :]
y_train_set = y[: math.floor(len(y) * 0.8), :]
x_validation = x[math.floor(len(x) * 0.8): , :]
y_validation = y[math.floor(len(y) * 0.8): , :]
# print(x_train_set)
# print(y_train_set)
# print(x_validation)
# print(y_validation)
# print(len(x_train_set))
# print(len(y_train_set))
# print(len(x_validation))
# print(len(y_validation))

features = 15 * 9 + 1
item = len(x_train_set)
w = np.zeros([features, 1]) # features*1 
x_train_set = np.concatenate((np.ones([item, 1]), x_train_set), axis=1).astype(float) # å¸¸æ•¸é …æ°´å¹³åˆä¸Š x_train_set 
x_validation = np.concatenate((np.ones([len(x_validation), 1]), x_validation), axis=1).astype(float)
learning_rate = 0.000001
iter_time = 10000
adagrad = np.zeros([features, 1])
eps = 0.0000000001 # /epsilon/
loss_array = []
for t in range(iter_time):
    gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
    # adagrad += gradient ** 2
    # w = w - learning_rate / np.sqrt(adagrad + eps) * gradient    
    w = w - learning_rate * gradient    
    loss = np.sqrt(np.sum(np.power(y_train_set - np.dot(x_train_set, w), 2))/item) # rmse (root-mean-square error)
    if loss > 100:
        loss_array.append(20) # æ•¸å€¼å¤©èŠ±æ¿ï¼Œç¹ªåœ–ç”¨
    else:
        loss_array.append(loss)
    if (not(t%1000)) | (t==iter_time-1):
        print('Iter_time = ', t, "Loss(error) = ", loss)
print('Training error rate: '+str(round(loss/np.mean(y_train_set)*100, 2))+'%')
'''
[Training]
1. å‰µé€ Linear Model: weight, bias
2. è£½ä½œLoss Functionä¾†è¡¡é‡Functionçš„é æ¸¬æº–åº¦
3. ä½¿ç”¨Gradient descentè¨ˆç®—weight, biaså°Loss Functionçš„å¾®åˆ†ï¼Œä»¥æ­¤æ–¹å¼æœ‰æ•ˆç‡åœ°èª¿æ•´åƒæ•¸ï¼Œ
   å¿«é€Ÿæ‰¾åˆ°æœ€å¥½çš„åƒæ•¸å€¼çµ„åˆ

ç¯„ä¾‹Codeèªªæ˜ï¼š
   ä¸‹é¢çš„ code æ¡ç”¨ Root Mean Square Error (å‡æ–¹æ ¹èª¤å·®)
   å› ç‚ºå¸¸æ•¸é …çš„å­˜åœ¨ï¼Œæ‰€ä»¥ dimension (dim) éœ€è¦å¤šåŠ ä¸€æ¬„ï¼›
   eps é …æ˜¯é¿å… adagrad çš„åˆ†æ¯ç‚º 0 è€ŒåŠ çš„æ¥µå°æ•¸å€¼ã€‚
   æ¯ä¸€å€‹ dimension (dim) æœƒå°æ‡‰åˆ°å„è‡ªçš„ gradient, weight (w)ï¼Œ
   é€éè¿­ä»£ (iter_time, iteration) èª¿æ•´åƒæ•¸ã€‚

# -----ä»¥ä¸‹ç­†è¨˜----- #

# å‡è¨­ Function Model
å‡è¨­é æ¸¬å€¼ç‚º Y_pï¼Œè¼¸å…¥çš„ 9å°æ™‚ * 15å€‹ Features ç‚º X_iï¼Œ
æ¯å€‹æ™‚ç©ºçš„ Feature éƒ½ç¨ç«‹ï¼Œæ•… i ç‚º 9 * 15 = 135
Y_p = bias + w1 * X1 + w2 * X2 + ... + w15 * X15
    = bias + Summation_i^135(w_i * X_i)

# Loss Function (æå¤±å‡½æ•¸) çš„è£½ä½œï¼šå¯ç”¨ä»»ä½•åˆç†çš„æ–¹å¼è©•æ–·Functioné æ¸¬æ•ˆæœçš„å„ªåŠ£
  èª²å ‚æ•™å°çš„æ–¹æ³•æ˜¯ç”¨é¡ä¼¼çµ±è¨ˆå­¸çš„ Variance (è®Šç•°æ•¸) åšè©•ä¼°
  * Variance è®Šç•°æ•¸ å°±æ˜¯æ¯ä¸€å€‹è§€å¯Ÿå€¼èˆ‡å¹³å‡æ•¸çš„çš„è·é›¢å¹³æ–¹å’Œçš„å¹³å‡
  * æ­¤å°‡ Variance çš„å¹³å‡é›¢å‡å·®ç‰¹æ€§è¦–ç‚ºã€Œä¼°è¨ˆèª¤å·®ã€ï¼Œè¼¸å…¥è³‡æ–™çš„å€‹æ•¸å°±æ˜¯ Training datas çš„ç¸½æ•¸ï¼Œ
    ç‰¹åˆ¥ä¹‹è™•åœ¨æ–¼é€™è£¡ Variance çš„ã€Œå‡æ•¸ã€ä¹Ÿå°±æ˜¯æŒ‡ä¼°è¨ˆèª¤å·®çš„Labelå€¼æ˜¯è¦éš¨è‘—ä¸åŒçµ„ Training data è€Œ
    è®Šå‹•çš„ï¼Œä¸¦éå‚³çµ±é›¢å‡å·®æœ‰ä¸€å€‹å›ºå®šçš„å‡å€¼ã€‚
  * ç¸½çµæ­¤ Loss Function è¼¸å…¥ (w, b) å³ Function çš„ä¸€çµ„ä¿‚æ•¸ã€Training datas çš„ Input åŠ Outputï¼Œ
    è¼¸å‡ºçš„ Loss å€¼ç‚ºã€ŒLabeläº‹å¯¦å’Œé æ¸¬çµæœä¹‹é–“çš„å‡æ–¹å·®ã€
  * ç›®æ¨™æ˜¯èª¿æ•´è¼¸å…¥çš„åƒæ•¸ (w, b) ï¼Œæ‰¾åˆ°å¤ å°çš„ Loss å€¼ï¼Œå°±æ˜¯æœ€ä½³ Functionåƒæ•¸ï¼Œ
    èª¿æ•´åƒæ•¸é€™ä»¶äº‹æƒ…ä¸ç”¨æ‰‹å‹•èª¿ï¼Œä¹Ÿä¸æ˜¯æš´åŠ›æ³•ç›´æ¥é–‹ï¼Œæ­¤é¡Œå¯ä»¥ç”¨ç·šæ€§ä»£æ•¸è§£å‡º Close Formï¼Œ
    ä¹Ÿèƒ½ä½¿ç”¨ Gradient descent ä¾†åšã€‚

# Gradient descent (æ¢¯åº¦ä¸‹é™æ³•)
  * ç›®çš„ï¼šç‚º Loss Function æ‰¾åˆ°æ¯”è¼ƒå¥½çš„åƒæ•¸ Wï¼Œä½¿ Loss(W) å€¼æœ€å°ã€‚
  * é™åˆ¶ï¼šæ‰€å®šç¾©çš„ Loss Function å¿…éœ€å¯å¾®åˆ†
  * å¯¦ä½œï¼šéš¨æ©Ÿä»¤ W åˆå€¼ï¼Œè¨ˆç®— W å° Loss Function çš„å¾®åˆ†å€¼ gdï¼Œ
    è‹¥ gd æ˜¯æ­£æ•¸ (æ–œç‡ç‚ºæ­£) ä¸‹ä¸€è¼ªå°±æ¸›å°‘ Wï¼Œåä¹‹ gd ç‚ºè² æ•¸ï¼Œå‰‡åœ¨ä¸‹ä¸€è¼ªå¢åŠ  W çš„å€¼ã€‚
  * å…¬å¼ï¼šè³¦äºˆä¸€å€‹ weight å€¼ (ç¨±ç‚ºLearning rateï¼Œä»£è™Ÿæ˜¯ Î· /Eta/) ä¾†èª¿æ•´ä¸‹ä¸€è¼ªè©²
    å¢åŠ /æ¸›å°‘ W å¤šå°‘å€¼ (ä»¥æ­¤æå‡èª¿æ•´åƒæ•¸çš„æ•ˆç‡)ã€‚ä»¤ W' ç‚ºä¸‹ä¸€è¼ªåƒæ•¸å€¼ï¼Œå…¬å¼ç‚º {W' = W - Eta * gd}
'''
'''
# ç¸½ Train
dim = 15 * 9 + 1 # å› ç‚ºæœ‰å¸¸æ•¸é …åƒæ•¸ biasï¼Œæ‰€ä»¥ dimension (dim) éœ€è¦å¤šåŠ ä¸€æ¬„
w = np.zeros([dim, 1]) # [dim, 1]å’Œ(dim, 1)ä¸€æ¨£æ„æ€ï¼Œå°±æ˜¯å­˜æˆ dim * 1 çš„äºŒç¶­é›¶çŸ©é™£
x = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float) # å› ç‚ºå¸¸æ•¸é …çš„å­˜åœ¨ï¼Œæ‰€ä»¥ dimension (dim) éœ€è¦å¤šåŠ ä¸€æ¬„
learning_rate = 0.000001 #ada:100 # å°±æ˜¯/Eta/ gradient descentçš„å¸¸ä¿‚æ•¸
iter_time = 1000 #1000 # K:gradient descentçš„è¿­ä»£æ¬¡æ•¸
adagrad = np.zeros([dim, 1]) # ??? K:å°±æ˜¯æ¯æ¬¡æ›´æ–°çš„ğœ‚å°±æ˜¯ç­‰æ–¼å‰ä¸€æ¬¡çš„ğœ‚å†é™¤ä»¥ğœ^tï¼Œè€Œ Ïƒ^tå‰‡ä»£è¡¨çš„æ˜¯ç¬¬ t æ¬¡ä»¥å‰çš„æ‰€æœ‰æ¢¯åº¦æ›´æ–°å€¼ä¹‹ root mean square (å¹³æ–¹å’Œé–‹æ ¹è™Ÿ)
eps = 0.0000000001 # /epsilon/
loss_array = []
# print('iter_time & Loss\n')
for t in range(iter_time):
    # è§£é‡‹æ¢¯åº¦ (gradient) çš„é‹ç®—ï¼š
    #   ä»¤ è³‡æ–™çš„æ•¸é‡ç‚º item = 8,892  
    #   ä»¤ features çš„æ•¸é‡ç‚º dim = 135
    #   ä»¤ è¨“ç·´è³‡æ–™ç‚º xï¼ŒäºŒç¶­çŸ©é™£ (item*dim)
    #   ä»¤ åƒæ•¸(æ¬Šé‡å€¼ï¼Œå…§æ¶µ Bias)ç‚º wï¼ŒäºŒç¶­çŸ©é™£ (dim*1) 
    #   ä»¤ Ground truth ç‚º yï¼ŒäºŒç¶­çŸ©é™£ (item*1)
    #   ä»¤ æ¢¯åº¦ ( w å° SSE (Sum Square Error, èª¤å·®å¹³æ–¹å’Œ) è¶¨è¿‘é›¶çš„å¾®åˆ†è¨ˆç®—çµæœ) ç‚º D = (np.dot(x, w) - y)ï¼ŒäºŒç¶­çŸ©é™£(item*1)
    #   ä»¤ è½‰ç½®çŸ©é™£é‹ç®—å­ç‚º ^T
    #   æ¢¯åº¦é‹ç®—çš„è½‰ç½®ç°¡åŒ–æ¨å°ï¼š2 * (çŸ©é™£D^T * çŸ©é™£x)^Tï¼Œå¾—åˆ°ä¸€å€‹ dim*1 çš„çµæœï¼Œä¾ç…§è½‰ç½®é‹ç®—åŒ–ç°¡è®Šæˆ 2*(çŸ©é™£x^T * çŸ©é™£D)
    gradient = 2 * np.dot(x.transpose(), (np.dot(x, w)-y)) #dim*1
    # if(t==iter_time-1):
    #     print('Gradient:\n', pd.DataFrame(gradient))
    
    # ä½¿ç”¨ AdaGrad
    #adagrad += gradient ** 2
    #w = w - learning_rate / np.sqrt(adagrad + eps) * gradient    
    w = w - learning_rate * gradient    
    # ç›®å‰çš„ Loss æ•¸å€¼
    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12) #rmse(root-mean-square deviation) #Loss Function #çŸ©é™£ç›¸ä¹˜: [(12*471)*(1+15*9)] dot [(1+15*9)*1]=[(12*471)*(1)]
    #loss = np.sum(np.power(np.dot(x, w) - y, 2))/(471*12)
    loss_array.append(loss)
    # if(t%100==0)|(t==iter_time-1):
    #     print('Iter_time = '+ str(t), "Loss(error) = " + str(loss))
np.save(r'LinearRegression\TrainingData_2019_Pinzhen\weight.npy', w)
w
# print('Function Parameter:\n', pd.DataFrame(w))
'''
'''
[Testing]
# é€™240é …è³‡æ–™æ‡‰è©²æ˜¯èƒå–å®Œå¾Œéš¨æ©Ÿå–å‡ºçš„ï¼Œè³‡æ–™çš„èƒå–å·¥ä½œæš«æ™‚è·³éï¼Œç›®å‰ç”¨Exceléš¨æ©Ÿé¸å–è³‡æ–™ä»£æ›¿
# æ¸¬è©¦è³‡æ–™ä¹Ÿè¦ç¶“éæ¨™æº–åŒ–è™•ç†æ‰èƒ½è¼¸å…¥ Function
è¼‰å…¥ test dataï¼Œä¸¦ä¸”ä»¥ç›¸ä¼¼æ–¼è¨“ç·´è³‡æ–™é å…ˆè™•ç†å’Œç‰¹å¾µèƒå–çš„æ–¹å¼è™•ç†ï¼Œ
ä½¿ test data å½¢æˆ 240 å€‹ç¶­åº¦ç‚º 15 * 9 + 1 çš„è³‡æ–™ã€‚
'''
testdata = pd.read_csv(r'LinearRegression\TestingData\EASY_TEST.csv', header = None, encoding = 'utf-8')

test_data = testdata.iloc[:, 1:10]
# test_data[test_data == 'NR'] = 0 # å·²å®Œæˆ
test_data = test_data.to_numpy()
test_x = np.empty([240, 15*9], dtype = float)
# std_x, mean_x è¦ä»¥è¨“ç·´è³‡æ–™çš„æ•¸å€¼é€²è¡Œæ¨™æº–åŒ–ï¼Œæ‰èƒ½å°‡æ¸¬è©¦è³‡æ–™è½‰æˆç›¸åŒçš„æ¯”ä¾‹å°ºé€²è¡Œé‹ç®— 
for i in range(240):
    test_x[i, :] = test_data[15 * i: 15* (i + 1), :].reshape(1, -1) # test_x æ˜¯æ¨™æº–äºŒç¶­é™£åˆ— 240*135
for i in range(len(test_x)): # äºŒç¶­é™£åˆ—çš„é•·åº¦æ˜¯ç®—æœ€å¤–æ¡†è£¡é¢å…§æ¶µçš„ä¸€ç¶­é™£åˆ—å€‹æ•¸ï¼Œå› æ­¤æ˜¯ 240ï¼Œå³è¼¸å…¥çš„æ¸¬è©¦è³‡æ–™é …ç›®å€‹æ•¸
    for j in range(len(test_x[0])): # 135 å€‹ Features
        if std_x[j] != 0: # æ ¹æ“šé™¤æ³•å®šè£¡ï¼Œåˆ†æ¯ä¸å¾—ç‚ºé›¶
            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]
test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)
test_x
#print(test_x)

# Ground truth of testing marks as test_y
test_GroundTruth = testdata.iloc[:, 10:]
test_GroundTruth = test_GroundTruth.to_numpy()
test_y = np.empty([240, 1], dtype = float)
for i in range(240):
    test_y[i][0] = test_GroundTruth[15*i+7, 0]
#print(pd.DataFrame(test_y))
'''
[Prediction]
ç¾åœ¨æˆ‘å€‘å·²å®šå‡º Model (é æ¸¬æ¨¡å‹, Functuon set)ã€
æ‰¾åˆ°è‡ªèªå®Œç¾ Function çš„ä¿‚æ•¸çµ„åˆã€é¸å¥½äº†æ¸¬è©¦è³‡æ–™é›†ï¼Œ
é‚£å°±èƒ½é€²è¡Œé æ¸¬äº†ï½
'''
w = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\weight.npy')
ans_y = np.dot(test_x, w) # [item*dim] * [dim*1] = [item*1] çŸ©é™£ç›¸ä¹˜çš„å¥§ç¥•å°±æ˜¯ï¼š
                          # è¢«å·¦ä¹˜çŸ©é™£ï¼Œå‰‡åˆ—é‹ç®—ï¼›è¢«å³ä¹˜ä¸€å€‹çŸ©é™£ï¼Œå‰‡è¡Œé‹ç®—ã€‚
                          # çŸ©é™£ç›¸ä¹˜æ˜¯ç›¸å°çš„æ¦‚å¿µï¼Œå–®çœ‹ä½ çš„ä¸»é«”æ˜¯èª°ã€‚
                          # ç”¨çŸ©é™£ç›¸ä¹˜å¯è¡¨ç¤ºæ–¹ç¨‹å¼çš„ã€Œä¿‚æ•¸èˆ‡æœªçŸ¥æ•¸ç›¸ä¹˜å†ç›¸åŠ ã€ï¼Œå³ç·šæ€§çµ„åˆ
ans_y
#print(ans_y)
'''
[Save Prediction to CSV File]
'''
import csv
with open(r'LinearRegression\PredictionResult\PredictionResult.csv', mode='w', newline='') as submit_file:
    csv_writer = csv.writer(submit_file)
    header = ['item_id', 'value']
    # print(header)
    csv_writer.writerow(header)
    for i in range(240):
        row = ['id_' + str(i), ans_y[i][0]]
        csv_writer.writerow(row)
        # print(row)
'''
[Functionçš„æ”¹å–„]
ç”¨å„å¼ Model æ¯”è¼ƒè¼¸å…¥ validation_set é ä¼°çµæœçš„ Average Error ä¾†é¸æ“‡æ›´å¥½çš„ Model
åˆ‡å‹¿ç”¨ Testing data ä¾†åšç¯©é¸ï¼Œå¦å‰‡æœƒå°è‡´ Model é æ¸¬ Private Test data çš„çµæœè®Šå·®
'''

'''
Loss è®ŠåŒ–åˆ†æ
'''
plt.axis([0, iter_time, 0, max(loss_array)+1])
x_pos = np.linspace(0, iter_time, iter_time)
y_pos = loss_array
# print(pd.DataFrame(y_pos))
plt.plot(x_pos, y_pos, '-', c='blue', markersize=4)
print('Train Ave_err: ', loss_array[-1]) # No AdaGrad, all testing data: 5.158543826472928 iter:1000 ETA:0.000001
print('Validation Ave_err: ', np.sqrt(np.sum((y_validation - np.dot(x_validation, w))**2)/len(y_validation)))
print('Testing Ave_err: ', np.sqrt(np.sum((test_y - np.dot(test_x, w))**2)/len(test_y)))
#plt.show()
