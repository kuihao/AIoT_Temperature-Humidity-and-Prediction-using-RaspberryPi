'''
é¡Œç›®ï¼šæ¡ƒåœ’å¹³é® PM 2.5 é æ¸¬
'''
import sys
import numpy as np 
import matplotlib.pyplot as plt #ç•«å‡ºåœ–å‹ 
import pandas as pd #è³‡æ–™è™•ç†
pd.set_option("display.max_rows", 1000)    #è¨­å®šæœ€å¤§èƒ½é¡¯ç¤º1000rows
pd.set_option("display.max_columns", 1000) #è¨­å®šæœ€å¤§èƒ½é¡¯ç¤º1000columns
'''
[Load Train Data(åŒ¯å…¥è¨“ç·´è³‡æ–™)]
TrainData_2019_PingZhen.csv çš„è³‡æ–™ç‚º 12 å€‹æœˆä¸­ï¼Œæ¯å€‹æœˆå– 20 å¤©ï¼Œæ¯å¤© 24 å°æ™‚çš„è³‡æ–™(æ¯å°æ™‚è³‡æ–™æœ‰ 18 å€‹ features)
'''
data = pd.read_csv('LinearRegression\TrainingData_2019_Pinzhen\TrainData_2019_PingZhen.csv', encoding = 'utf-8')
'''
[Preprocessing(è³‡æ–™é è™•ç†)]
Column Name 'RAINFALL' çš„ NR(No Rain) å…¨éƒ¨æ”¹æˆ 0 ......[å·²å®Œæˆ]
'''
data = data.iloc[:, 3:]
# data[data == 'NR'] = 0
raw_data = data.to_numpy()
'''
[Extract Features (1)] ä»¥ 12 å€‹æœˆçš„ 15 å€‹Features (å…±180é …) ç‚ºåˆ—ï¼Œå°æ™‚ç‚ºæ¬„
å°‡åŸå§‹æ•¸æ“š 3600 (ç­†) * 24 (hours) ä¾ç…§æœˆä»½é‡æ–°åˆ†çµ„æˆ 12 å€‹ 15 (features) * 480 (hours) çš„è³‡æ–™
'''
month_data = {}
for month in range(12):
    sample = np.empty([15, 480])
    for day in range(20):
        # sampleæ˜¯å®¹å™¨ï¼Œ15 featuresç‚ºåˆ—ï¼Œä¸€æ—¥24å°æ™‚ç‚ºä¸€æ¬„ï¼Œè¦è£20å¤©ï¼›row_dataæ¯æ¬¡é¸15ç­†feature(å«24æ¬„)ï¼Œä¸€å€‹æœˆæœ‰20å¤©çš„è³‡æ–™*ç›®å‰ç¬¬å¹¾å€‹æœˆ+å·²ç®—åˆ°ç¬¬å¹¾å¤©
        sample[:, day * 24 : (day + 1) * 24] = raw_data[15 * (20 * month + day) : 15 * (20 * month + day + 1), :]
    month_data[month] = sample
'''
[Extract Features (2)]æŠ½å‡ºè¨“ç·´è³‡æ–™
1. æ¯å€‹æœˆæœ‰ 15 (features) * 480 (hours) çš„æ•¸æ“šã€‚
2. å¾ä¸­ã€Œç”¢ç”Ÿè¨“ç·´è³‡æ–™ã€çš„è¦å‰‡æ˜¯ï¼š
   æ¯ 10 å€‹å°æ™‚çš„è³‡æ–™çµ„æˆä¸€ç­†è¨“ç·´è³‡æ–™ (æ„æ€æ˜¯åˆå¤œ12é»è‡³æ—©ä¸Š10é»ç‚ºç¬¬ä¸€ç­†ï¼Œå‡Œæ™¨1é»è‡³æ—©ä¸Š11é»ç‚ºç¬¬äºŒç­†ï¼Œä¾æ­¤é¡æ¨)ï¼Œ
   å…¶ä¸­ï¼Œå‰ 9 å°æ™‚çš„è³‡æ–™ä½œç‚º Input Datasï¼Œæœ€å¾Œ 1 å°æ™‚çš„ PM 2.5 æ•¸å€¼ä½œç‚º Output Dataï¼Œ
   å› æ­¤ï¼ŒInput Datas ç‚º 15 (features) * 9 (hours)ï¼ŒOutput Data ç‚º 1 (features) * 1 (hours)ã€‚
3. æ¯å€‹æœˆæ‰£é™¤æœ€å¾Œ9å°æ™‚è³‡æ–™ä¸æ‹¿ä¾†é æ¸¬(ç‚ºä½•ä¸æ˜¯ä¸€å¹´ç‚ºè¨ˆç®—ï¼Ÿ)ï¼Œå‰‡å¯é æ¸¬è³‡æ–™é‡æ˜¯ 480 - 9 = 471 å€‹å°æ™‚çš„ PM 2.5ï¼Œ
   ä¹Ÿå°±æ˜¯æ¯å€‹æœˆå¯å¾ä¸­æå– 471 ç­†è¨“ç·´è³‡æ–™ï¼Œæ›ç®—æˆæ•´å¹´å°±æ˜¯ 12 * 471 = 8,892 ç­†è¨“ç·´è³‡æ–™ã€‚
'''
# x ç‚ºè¨“ç·´è³‡æ–™çš„è¼¸å…¥ï¼Œåˆ—æ˜¯è¨“ç·´è³‡æ–™å€‹æ•¸(ä¸€å€‹æœˆ471ç­†ï¼Œä¹˜ä¸Š12å€‹æœˆ)ï¼Œæ¬„æ˜¯9å€‹å°æ™‚çš„Featureså¾ªç’°
x = np.empty([12 * 471, 15 * 9], dtype = float)
# y ç‚ºè¨“ç·´è³‡æ–™çš„è¼¸å‡ºï¼Œåˆ—æ˜¯è¨“ç·´è³‡æ–™å€‹æ•¸(ä¸€å€‹æœˆ471ç­†ï¼Œä¹˜ä¸Š12å€‹æœˆ)ï¼Œæ¬„æ˜¯ä¸‹ä¸€å°æ™‚çš„PM 2.5
y = np.empty([12 * 471, 1], dtype = float)
for month in range(12):
    for day in range(20):
        for hour in range(24):
            # ç”±æ–¼æ²’æœ‰éš”å¹´å¹´åˆçš„è³‡æ–™ï¼Œå› æ­¤å¹´åº•å‰©é¤˜çš„9å€‹å°æ™‚çš†ç„¡æ³•æ§‹æˆè¨“ç·´è³‡æ–™
            if day == 19 and hour > 14:
                continue
            # æ¯ä¸€ç­†è¨“ç·´è³‡æ–™æ˜¯æ–°å¢åˆ—(é‰›ç›´è»¸æ–°å¢)
            # x æ˜¯è¼¸å…¥ç«¯ï¼Œå– month_data [æœˆ][ 15å€‹featureså…¨è¦, éå¢çš„9å€‹å°æ™‚(æ¬„çš„ç¸½é•·åº¦å–®ä½æ˜¯20å¤©æ›ç®—å°æ™‚è¨ˆ)]
            # ç¶“éreshapeï¼Œnumphyå¯ä»¥å»£æ’­è‡³xçŸ©é™£ï¼Œä¸”åŒé¡çš„featureæœƒæ¥çºŒæ’åˆ—
            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)
            # y æ˜¯è¼¸å…¥ç«¯ï¼Œå– month_data [æœˆ][ PM 2.5çš„é …, ç¬¬10å€‹å°æ™‚(indexèµ·æ–¼0ï¼Œæ•…ç¬¬10å°æ™‚ä»¥9èµ·)]
            y[month * 471 + day * 24 + hour, 0] = month_data[month][7, day * 24 + hour + 9] #value
# print(x)
# print(y)
'''
[Normalize (1)]
æ¨™æº–åŒ–ï¼šæ¸¬é‡ä¸€çµ„æ•¸å€¼çš„é›¢æ•£ç¨‹åº¦ä½¿ç”¨ mean(x) å¹³å‡å€¼ã€Standard Deviation æ¨™æº–å·®
æ­£æ­¸åŒ–ï¼šx_normalized = x - min(x) / max(x) - min(x)
'''
mean_x = np.mean(x, axis = 0) #15 * 9: axis = 0 æ˜¯å–æ¬„æ–¹å‘12 * 471çš„è³‡æ–™ç®— Meanï¼Œå› æ­¤æœƒæœ‰ 15 * 9 å€‹å€¼ 
std_x = np.std(x, axis = 0) #15 * 9 
for i in range(len(x)): #12 * 471
    for j in range(len(x[0])): #15 * 9 
        if std_x[j] != 0:
            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]
x
# print(x)
'''
[Split Training Data Into "train_set" and "validation_set"]
train_setç”¨ä¾†è¨“ç·´ï¼Œvalidation_setä¸æœƒè¢«æ”¾å…¥è¨“ç·´ã€åªæ˜¯ç”¨ä¾†é©—è­‰
'''
import math
x_train_set = x[: math.floor(len(x) * 0.8), :]
y_train_set = y[: math.floor(len(y) * 0.8), :]
x_validation = x[math.floor(len(x) * 0.8): , :]
y_validation = y[math.floor(len(y) * 0.8): , :]
#print(x_train_set)
#print(y_train_set)
#print(x_validation)
#print(y_validation)
#print(len(x_train_set))
#print(len(y_train_set))
#print(len(x_validation))
#print(len(y_validation))
'''
[Training]
1. é€ Linear Model: weight, bias
2. åšä¸€å€‹Loss Functionä¾†é¸Function
3. Loss Functionæ­é…Gradient descentï¼Œè¨ˆç®—weight, biaså°Functionçš„å¾®åˆ†ä¾†æ‰¾åˆ°æœ€å¥½çš„åƒæ•¸å€¼

ä¸‹é¢çš„ code æ¡ç”¨ Root Mean Square Error
å› ç‚ºå¸¸æ•¸é …çš„å­˜åœ¨ï¼Œæ‰€ä»¥ dimension (dim) éœ€è¦å¤šåŠ ä¸€æ¬„ï¼›
eps é …æ˜¯é¿å… adagrad çš„åˆ†æ¯ç‚º 0 è€ŒåŠ çš„æ¥µå°æ•¸å€¼ã€‚
æ¯ä¸€å€‹ dimension (dim) æœƒå°æ‡‰åˆ°å„è‡ªçš„ gradient, weight (w)ï¼Œ
é€éä¸€æ¬¡æ¬¡çš„ iteration (iter_time) å­¸ç¿’ã€‚

# -----ä»¥ä¸‹ç­†è¨˜----- #

# å‡è¨­ Function Model
å‡è¨­é æ¸¬å€¼ç‚º Y_pï¼Œè¼¸å…¥çš„ 9å°æ™‚ * 15å€‹ Features ç‚º X_iï¼Œ
æ¯å€‹æ™‚ç©ºçš„ Feature éƒ½ç¨ç«‹ï¼Œæ•… i ç‚º 9 * 15 = 135
Y_p = bias + w1 * X1 + w2 * X2 + ... + w15 * X15
    = bias + Summation_i^135(w_i * X_i)

# Loss Function (æå¤±å‡½æ•¸) çš„è£½ä½œï¼šå¯ç”¨ä»»ä½•åˆç†çš„æ–¹å¼è©•æ–·Functioné æ¸¬æ•ˆæœçš„å„ªåŠ£
  èª²å ‚æ•™å°çš„æ–¹æ³•æ˜¯ç”¨é¡ä¼¼çµ±è¨ˆå­¸çš„ Variance (è®Šç•°æ•¸) åšè©•ä¼°
  * Variance è®Šç•°æ•¸ å°±æ˜¯æ¯ä¸€å€‹è§€å¯Ÿå€¼èˆ‡å¹³å‡æ•¸çš„çš„è·é›¢å¹³æ–¹å’Œçš„å¹³å‡
  * æ­¤å°‡ Variance çš„å¹³å‡é›¢å‡å·®ç‰¹æ€§è¦–ç‚ºã€Œä¼°è¨ˆèª¤å·®ã€ï¼Œè¼¸å…¥è³‡æ–™çš„å€‹æ•¸å°±æ˜¯ Training datas çš„ç¸½æ•¸ï¼Œ
    ç‰¹åˆ¥ä¹‹è™•åœ¨æ–¼é€™è£¡ Variance çš„ã€Œå‡æ•¸ã€ä¹Ÿå°±æ˜¯æŒ‡ä¼°è¨ˆèª¤å·®çš„Labelå€¼æ˜¯è¦éš¨è‘—ä¸åŒçµ„ Training data è€Œ
    è®Šå‹•çš„ï¼Œä¸¦éå‚³çµ±é›¢å‡å·®æœ‰ä¸€å€‹å›ºå®šçš„å‡å€¼ã€‚
  * ç¸½çµæ­¤ Loss Function è¼¸å…¥ (w, b) å³ Function çš„ä¸€çµ„ä¿‚æ•¸ã€Training datas çš„ Input åŠ Outputï¼Œ
    è¼¸å‡ºçš„ Loss å€¼ç‚ºã€ŒLabeläº‹å¯¦å’Œé æ¸¬çµæœä¹‹é–“çš„å‡æ–¹å·®ã€
  * ç›®æ¨™æ˜¯èª¿æ•´è¼¸å…¥çš„åƒæ•¸ (w, b) ï¼Œæ‰¾åˆ°å¤ å°çš„ Loss å€¼ï¼Œå°±æ˜¯æœ€ä½³ Functionåƒæ•¸ï¼Œ
    èª¿æ•´åƒæ•¸é€™ä»¶äº‹æƒ…ä¸ç”¨æ‰‹å‹•èª¿ï¼Œä¹Ÿä¸æ˜¯æš´åŠ›æ³•ç›´æ¥é–‹ï¼Œæ­¤é¡Œå¯ä»¥ç”¨ç·šæ€§ä»£æ•¸è§£å‡º Close Formï¼Œ
    ä¹Ÿèƒ½ä½¿ç”¨ Gradient descent ä¾†åšã€‚

# Gradient descent (æ¢¯åº¦ä¸‹é™æ³•)
  * ç›®çš„ï¼šç‚º Loss Function æ‰¾åˆ°æ¯”è¼ƒå¥½çš„åƒæ•¸ Wï¼Œä½¿ Loss(W) å€¼æœ€å°ã€‚
  * é™åˆ¶ï¼šæ‰€å®šç¾©çš„ Loss Function å¿…éœ€å¯å¾®åˆ†
  * å¯¦ä½œï¼šéš¨æ©Ÿä»¤ W åˆå€¼ï¼Œè¨ˆç®— W å° Loss Function çš„å¾®åˆ†å€¼ dï¼Œ
    è‹¥ d æ˜¯æ­£æ•¸ (æ–œç‡ç‚ºæ­£) ä¸‹ä¸€è¼ªå°±æ¸›å°‘ Wï¼Œåä¹‹ d ç‚ºè² æ•¸ï¼Œå‰‡åœ¨ä¸‹ä¸€è¼ªå¢åŠ  W çš„å€¼ã€‚
  * å…¬å¼ï¼šè³¦äºˆä¸€å€‹ weight å€¼ (ç¨±ç‚ºLearning rateï¼Œä»£è™Ÿæ˜¯ Î· /Eta/) ä¾†èª¿æ•´ä¸‹ä¸€è¼ªè©²
    å¢åŠ /æ¸›å°‘ W å¤šå°‘å€¼ (ä»¥æ­¤æå‡èª¿æ•´åƒæ•¸çš„æ•ˆç‡)ã€‚ä»¤ W' ç‚ºä¸‹ä¸€è¼ªåƒæ•¸å€¼ï¼Œå…¬å¼ç‚º {W' = W - Eta * d}
'''
dim = 15 * 9 + 1 # å› ç‚ºå¸¸æ•¸é …çš„å­˜åœ¨ï¼Œæ‰€ä»¥ dimension (dim) éœ€è¦å¤šåŠ ä¸€æ¬„
w = np.zeros([dim, 1]) 
x = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float) # å› ç‚ºå¸¸æ•¸é …çš„å­˜åœ¨ï¼Œæ‰€ä»¥ dimension (dim) éœ€è¦å¤šåŠ ä¸€æ¬„
learning_rate = 100 # K:gradient descent
iter_time = 1000 # K:gradient descent
adagrad = np.zeros([dim, 1]) # K:å°±æ˜¯æ¯æ¬¡æ›´æ–°çš„ğœ‚å°±æ˜¯ç­‰æ–¼å‰ä¸€æ¬¡çš„ğœ‚å†é™¤ä»¥ğœ^tï¼Œè€Œ Ïƒ^tå‰‡ä»£è¡¨çš„æ˜¯ç¬¬ t æ¬¡ä»¥å‰çš„æ‰€æœ‰æ¢¯åº¦æ›´æ–°å€¼ä¹‹å¹³æ–¹å’Œé–‹æ ¹è™Ÿ(root mean square)
eps = 0.0000000001
#print('iter_time & Loss\n')
for t in range(iter_time):
    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12)#rmse
    #if(t%100==0):
    #    print(str(t) + ":" + str(loss))
    gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) #dim*1
    adagrad += gradient ** 2
    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)
np.save('weight.npy', w)
w
'''
[Testing]
# é€™240é …è³‡æ–™æ‡‰è©²æ˜¯èƒå–å®Œå¾Œéš¨æ©Ÿå–å‡ºçš„ï¼Œè³‡æ–™çš„èƒå–å·¥ä½œæš«æ™‚è·³éï¼Œç›®å‰ç”¨Exceléš¨æ©Ÿé¸å–è³‡æ–™ä»£æ›¿
è¼‰å…¥ test dataï¼Œä¸¦ä¸”ä»¥ç›¸ä¼¼æ–¼è¨“ç·´è³‡æ–™é å…ˆè™•ç†å’Œç‰¹å¾µèƒå–çš„æ–¹å¼è™•ç†ï¼Œ
ä½¿ test data å½¢æˆ 240 å€‹ç¶­åº¦ç‚º 15 * 9 + 1 çš„è³‡æ–™ã€‚
'''
testdata = pd.read_csv('LinearRegression\TrainingData_2019_Pinzhen\EASY_TEST.csv', header = None, encoding = 'utf-8')
test_data = testdata.iloc[:, 1:10]
# test_data[test_data == 'NR'] = 0 # å·²å®Œæˆ
test_data = test_data.to_numpy()
test_x = np.empty([240, 15*9], dtype = float)
for i in range(240):
    test_x[i, :] = test_data[15 * i: 15* (i + 1), :].reshape(1, -1)
for i in range(len(test_x)):
    for j in range(len(test_x[0])):
        if std_x[j] != 0:
            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]
test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)
test_x
#print(test_x)
'''
[Prediction]
ç¾åœ¨æˆ‘å€‘å·²å®šå‡ºModel(é æ¸¬æ¨¡å‹, Functuon set)ã€
æ‰¾åˆ°è‡ªèªå®Œç¾Functionçš„ä¿‚æ•¸çµ„åˆã€é¸å¥½äº†æ¸¬è©¦è³‡æ–™é›†ï¼Œ
é‚£å°±èƒ½é€²è¡Œé æ¸¬äº†ï½
'''
w = np.load('weight.npy')
ans_y = np.dot(test_x, w)
ans_y
#print(ans_y)
'''
[Save Prediction to CSV File]
'''
import csv
with open('PredictionResult.csv', mode='w', newline='') as submit_file:
    csv_writer = csv.writer(submit_file)
    header = ['id', 'value']
    print(header)
    csv_writer.writerow(header)
    for i in range(240):
        row = ['id_' + str(i), ans_y[i][0]]
        csv_writer.writerow(row)
        print(row)
'''
[Functionçš„æ”¹å–„]
ç”¨å„å¼ Model æ¯”è¼ƒè¼¸å…¥ validation_set é ä¼°çµæœçš„ Average Error ä¾†é¸æ“‡æ›´å¥½çš„ Model
åˆ‡å‹¿ç”¨ Testing data ä¾†åšç¯©é¸ï¼Œå¦å‰‡æœƒå°è‡´ Model é æ¸¬ Private Test data çš„çµæœè®Šå·®
'''