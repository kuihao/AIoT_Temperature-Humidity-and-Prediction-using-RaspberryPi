'''
é¡Œç›®ï¼šæ¡ƒåœ’å¹³é® PM 2.5 é æ¸¬
ç¨‹å¼èªªæ˜ï¼šé›™äº•å­—ã€Œ##ã€è¡¨ç¤ºè³‡æ–™é è™•ç†ä¹‹å¿…è¦ç¨‹å¼ç¢¼ï¼Œåªæ˜¯ç‚ºæå‡å¯¦é©—æ•ˆç‡è€Œå±è”½
'''
import sys
import numpy as np 
import matplotlib.pyplot as plt #ç•«å‡ºåœ–å‹ 
import pandas as pd #è³‡æ–™è™•ç†
import csv
import gc
# pd.set_option("display.max_rows", 1000)    #è¨­å®šæœ€å¤§èƒ½é¡¯ç¤º1000rows
# pd.set_option("display.max_columns", 1000) #è¨­å®šæœ€å¤§èƒ½é¡¯ç¤º1000columns

'''
[Load Train Data(åŒ¯å…¥è¨“ç·´è³‡æ–™)]
TrainData_2019_PingZhen.csv çš„è³‡æ–™ç‚º 12 å€‹æœˆä¸­ï¼Œæ¯å€‹æœˆå– 20 å¤©ï¼Œæ¯å¤© 24 å°æ™‚çš„è³‡æ–™(æ¯å°æ™‚è³‡æ–™æœ‰ 15 å€‹ features)
'''
data = pd.read_csv(r'LinearRegression\TrainingData_2019_Pinzhen\TrainData_2019_PingZhen.csv', encoding = 'utf-8')
'''
[Preprocessing(è³‡æ–™é è™•ç†)]
Column Name 'RAINFALL' çš„ NR(No Rain) å…¨éƒ¨æ”¹æˆ 0 ......[å·²å®Œæˆ]
'''
data = data.iloc[:, 3:]
# data[data == 'NR'] = 0
raw_data = data.to_numpy()
'''
[Extract Features (1)] ä»¥ 12 å€‹æœˆçš„ 15 å€‹ Features (å…±180é …) ç‚ºåˆ—ï¼Œå°æ™‚ç‚ºæ¬„
å°‡åŸå§‹æ•¸æ“š 3600 (ç­†) * 24 (hours) ä¾ç…§æœˆä»½é‡æ–°åˆ†çµ„æˆ 12 å€‹ 15 (features) * 480 (hours) çš„è³‡æ–™
'''
month_data = {}
for month in range(12):
    sample = np.empty([15, 480])
    for day in range(20):
        # sampleæ˜¯å®¹å™¨ï¼Œ15 featuresç‚ºåˆ—ï¼Œä¸€æ—¥24å°æ™‚ç‚ºä¸€æ¬„ï¼Œè¦è£20å¤©ï¼›row_dataæ¯æ¬¡é¸15ç­†feature(å«24æ¬„)ï¼Œä¸€å€‹æœˆæœ‰20å¤©çš„è³‡æ–™*ç›®å‰ç¬¬å¹¾å€‹æœˆ+å·²ç®—åˆ°ç¬¬å¹¾å¤©
        sample[:, day * 24 : (day + 1) * 24] = raw_data[15 * (20 * month + day) : 15 * (20 * month + day + 1), :]
    month_data[month] = sample
'''
[Extract Features (2)]æŠ½å‡ºè¨“ç·´è³‡æ–™
1. æ¯å€‹æœˆæœ‰ 15 (features) * 480 (hours) çš„æ•¸æ“šã€‚
2. å¾ä¸­ã€Œç”¢ç”Ÿè¨“ç·´è³‡æ–™ã€çš„è¦å‰‡æ˜¯ï¼š
   æ¯ 10 å€‹å°æ™‚çš„è³‡æ–™çµ„æˆä¸€ç­†è¨“ç·´è³‡æ–™ (æ„æ€æ˜¯åˆå¤œ12é»è‡³æ—©ä¸Š10é»ç‚ºç¬¬ä¸€ç­†ï¼Œå‡Œæ™¨1é»è‡³æ—©ä¸Š11é»ç‚ºç¬¬äºŒç­†ï¼Œä¾æ­¤é¡æ¨)ï¼Œ
   å…¶ä¸­ï¼Œå‰ 9 å°æ™‚çš„è³‡æ–™ä½œç‚º Input Datasï¼Œæœ€å¾Œ 1 å°æ™‚çš„ PM 2.5 æ•¸å€¼ä½œç‚º Output Dataï¼Œ
   å› æ­¤ï¼ŒInput Datas ç‚º 15 (features) * 9 (hours)ï¼ŒOutput Data ç‚º 1 (features) * 1 (hours)ã€‚
3. æ¯å€‹æœˆæ‰£é™¤æœ€å¾Œ9å°æ™‚è³‡æ–™ä¸æ‹¿ä¾†é æ¸¬(ç‚ºä½•ä¸æ˜¯ä¸€å¹´ç‚ºè¨ˆç®—ï¼Ÿ)ï¼Œå‰‡å¯é æ¸¬è³‡æ–™é‡æ˜¯ 480 - 9 = 471 å€‹å°æ™‚çš„ PM 2.5ï¼Œ
   ä¹Ÿå°±æ˜¯æ¯å€‹æœˆå¯å¾ä¸­æå– 471 ç­†è¨“ç·´è³‡æ–™ï¼Œæ›ç®—æˆæ•´å¹´å°±æ˜¯ 12 * 471 = 5,652 ç­†è¨“ç·´è³‡æ–™ã€‚
'''
# x ç‚ºè¨“ç·´è³‡æ–™çš„è¼¸å…¥ï¼Œåˆ—æ˜¯è¨“ç·´è³‡æ–™å€‹æ•¸(ä¸€å€‹æœˆ471ç­†ï¼Œä¹˜ä¸Š12å€‹æœˆ)ï¼Œæ¬„æ˜¯9å€‹å°æ™‚çš„ Features å¾ªç’°
x = np.empty([12 * 471, 15 * 9], dtype = float)
# y ç‚ºè¨“ç·´è³‡æ–™çš„è¼¸å‡ºï¼Œåˆ—æ˜¯è¨“ç·´è³‡æ–™å€‹æ•¸(ä¸€å€‹æœˆ471ç­†ï¼Œä¹˜ä¸Š12å€‹æœˆ)ï¼Œæ¬„æ˜¯ä¸‹ä¸€å°æ™‚çš„PM 2.5
y = np.empty([12 * 471, 1], dtype = float)
for month in range(12):
    for day in range(20):
        for hour in range(24):
            # ç”±æ–¼æ¯å€‹æœˆçš„è¨“ç·´é›†è³‡æ–™ä¸¦ç„¡ç›¸é€£ï¼Œå› æ­¤æœ€å¾Œä¸€å¤©å‰©é¤˜çš„9å€‹å°æ™‚çš†ç„¡æ³•æ§‹æˆè¨“ç·´è³‡æ–™
            if day == 19 and hour > 14:
                continue
            # æ¯ä¸€ç­†è¨“ç·´è³‡æ–™æ˜¯æ–°å¢åˆ—(é‰›ç›´è»¸æ–°å¢)
            # x æ˜¯è¼¸å…¥ç«¯ï¼Œå– month_data [æœˆ][ 15 å€‹ features å…¨è¦, éå¢çš„ 9 å€‹å°æ™‚(æ¬„çš„ç¸½é•·åº¦å–®ä½æ˜¯ 20 å¤©æ›ç®—å°æ™‚è¨ˆ)]
            # ç¶“éreshapeï¼Œnumphyå¯ä»¥å»£æ’­è‡³xçŸ©é™£ï¼Œä¸”åŒé¡çš„featureæœƒæ¥çºŒæ’åˆ—
            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector feature_weights:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)
            # y æ˜¯è¼¸å‡ºç«¯ï¼Œå– month_data [æœˆ][ PM 2.5 çš„é …, ç¬¬ 10 å€‹å°æ™‚(indexèµ·æ–¼ 0 ï¼Œæ•…ç¬¬ 10 å°æ™‚ä»¥ 9 èµ·)]
            y[month * 471 + day * 24 + hour, 0] = month_data[month][7, day * 24 + hour + 9] #value
# print(x)
# print(y)

'''
[Shuffle] å°‡è³‡æ–™æ‰“æ•£éš¨æ©Ÿæ’åº
ç›®çš„ï¼šä½¿å¾ŒçºŒåˆ‡å‰² train_setã€validation_set è³‡æ–™å¯ä»¥è¼ƒç‚ºå¹³å‡
'''
box = np.concatenate((x, y), axis=1)
box = pd.DataFrame(box)
#print(box[0][0])
box = box.sample(frac=1).reset_index(drop=True) # n=len(box.index)
#print(box[0][0])
x = np.array(box.iloc[:,0:135])
y = np.array(box.iloc[:,135:])
#np.save(r'LinearRegression\TrainingData_2019_Pinzhen\x_tran_shuffle2.npy', x)
#np.save(r'LinearRegression\TrainingData_2019_Pinzhen\y_tran_shuffle2.npy', y)
'''
[Normalize (1)]
æ­£æ­¸åŒ–ï¼šç•¶è³‡æ–™ä¸æ˜¯æ•¸å€¼ï¼Œæˆ–æ˜¯é›–ç‚ºæ•¸å€¼å»æ˜¯é¡åˆ¥è³‡æ–™(å±¬æ€§è³‡æ–™ã€åç¾©å°ºåº¦)ï¼Œå‰‡éœ€ç¶“éç·¨ç¢¼è™•ç†
       (æœ¬å°ˆé¡Œå› ç‚ºä¸ç”¨ä½œæ•˜è¿°ã€æ¨è«–çµ±è¨ˆåˆ†æï¼Œå› æ­¤ä¸éœ€è¦)
æ¨™æº–åŒ–ï¼šæ¸¬é‡ä¸€çµ„æ•¸å€¼çš„é›¢æ•£ç¨‹åº¦ä½¿ç”¨ mean(x) å¹³å‡å€¼ã€Standard Deviation æ¨™æº–å·®
    æ–¹æ³•1 Max-Min å¸¸è¦‹çš„è³‡æ–™æ¨™æº–åŒ–æ–¹æ³•ï¼Œç°¡å–®ä¾†èªªï¼Œå°‡åŸå§‹è³‡æ–™çš„æœ€å¤§ã€æœ€å°å€¼mappingè‡³å€é–“[0,1]
          å…¬å¼ï¼šx_normalized = x - min(x) / max(x) - min(x)
    æ–¹æ³•2 æ¡ç”¨Z-Score (Zåˆ†æ•¸) åˆ©ç”¨åŸå§‹è³‡æ–™çš„å‡å€¼ï¼ˆmeanï¼‰å’Œæ¨™æº–å·®ï¼ˆstandard deviationï¼‰é€²è¡Œ
          è³‡æ–™çš„æ¨™æº–åŒ–ï¼Œé©ç”¨æ–¼è³‡æ–™çš„æœ€å¤§å€¼å’Œæœ€å°å€¼æœªçŸ¥çš„æƒ…æ³ï¼Œæˆ–æœ‰è¶…å‡ºå–å€¼ç¯„åœçš„é›¢ç¾¤è³‡æ–™çš„æƒ…æ³ã€‚
          å…¬å¼ï¼šæ–°è³‡æ–™ =ï¼ˆåŸå§‹è³‡æ–™-å‡å€¼ï¼‰/ æ¨™æº–å·®
'''
Normalization_Method = 'Z-Score' # 'Z-Score'
x = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\x_tran_shuffle2.npy')
y = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\y_tran_shuffle2.npy')
# Z-Score
mean_x = np.mean(x, axis = 0) #15 * 9: axis = 0 æ˜¯å–é‰›ç›´æ–¹å‘ï¼Œå°12 * 471çš„è³‡æ–™ç®— Meanï¼Œå› æ­¤æœƒæœ‰ 15 * 9 å€‹Meanå€¼ 
std_x = np.std(x, axis = 0) #15 * 9 
for i in range(len(x)): #12 * 471
    for j in range(len(x[0])): #15 * 9 
        if std_x[j] != 0:
            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]
            # ç›®å‰æ˜¯å°‡ 15 * 9 å€‹ Featureséƒ½è¦–ç‚ºç¨ç«‹ä¸åŒç¨®é¡çš„ Feature
            # z-score (åˆç¨±Standard Score) çš„æ¨™æº–åŒ–æ–¹æ³•å°±æ˜¯å°‡å€‹åˆ¥ Feature éƒ½å¾ç›®å‰çš„è³‡æ–™é›†è£¡é¢ï¼Œè¨ˆç®—è‡ªå·±é€™ç¨® Feature çš„
            # å¹³å‡æ•¸åŠæ¨™æº–å·®ï¼Œå…¬å¼ã€Œé›¢å‡å·®èˆ‡æ¨™æº–å·®çš„æ¯”å€¼ã€å°±æ˜¯æ¯çµ„è³‡æ–™èˆ‡å¹³å‡æ•¸çš„æ¨™æº–åŒ–å¾Œã€Œç›¸å°è·é›¢ã€
            # ä¾ç…§ä¸­å¤®æ¥µé™å®šç†ï¼Œä¸–ç•Œä¸Šæ‰€æœ‰è³‡æ–™çš„åˆ†å¸ƒ99.9936%éƒ½æœƒåŒ…åœ¨å››å€‹æ¨™æº–å·®ä¹‹å…§ï¼Œå› æ­¤
            # z-score å…¬å¼å¯ä»¥é”åˆ°ä¸åŒç¨®é¡è³‡æ–™çµ±ä¸€æ¨™æº–åŒ–å‘ˆç¾çš„æ•ˆæœ
# Max-Min
## max_x = np.max(x)
## min_x = np.min(x)
## full_range_x = max_x - min_x
## if full_range_x < 0:
##   full_range_x *= (-1) 
## for i in range(len(x)): #12 * 471
##     for j in range(len(x[0])): #15 * 9 
##       if x[i][j] - min_x < 0:
##         x[i][j] = (x[i][j] - min_x) * (-1)
##       else:
##         x[i][j] = (x[i][j] - min_x) / full_range_x
# print(x)
'''
[Split Training Data]
train_setç”¨ä¾†è¨“ç·´ï¼Œvalidation_setä¸æœƒè¢«æ”¾å…¥"train_set" and "validation_set"è¨“ç·´ã€åªæ˜¯ç”¨ä¾†é©—è­‰
'''
# ä»¥ä¸‹ç‚ºæ­£å¼å…¨éƒ¨çš„è¨“è³‡
## x_train_set = x
## y_train_set = y
# ä»¥ä¸‹ç‚º Validation
import math
x_train_set = x[: math.floor(len(x) * 0.8), :]
y_train_set = y[: math.floor(len(y) * 0.8), :]
x_validation = x[math.floor(len(x) * 0.8): , :]
y_validation = y[math.floor(len(y) * 0.8): , :]
# print(x_train_set)
# print(y_train_set)
# print(x_validation)
# print(y_validation)
# print(len(x_train_set)) # 4521
# print(len(y_train_set)) # 4521
# print(len(x_validation)) # 1131
# print(len(y_validation)) # 1131

import random
features = 15 * 9 + 1
item = len(x_train_set)
w = np.zeros([features, 1]) # features*1 
x_train_set = np.concatenate((np.ones([item, 1]), x_train_set), axis=1).astype(float) # å¸¸æ•¸é …æ°´å¹³åˆä¸Š x_train_set 
x_validation = np.concatenate((np.ones([len(x_validation), 1]), x_validation), axis=1).astype(float)
# ä»¥ä¸‹ç‚ºèª¿æ•´ Gradient çš„é‡è¦åƒæ•¸ 
SavingDialog = True #False
Model = 'one adjusting' 
Gradient_Method = 'AdaGrad'
learning_rate = 1
iter_time = 10000
#iter_time = math.ceil(150000/4521)
# AdaGrad åƒæ•¸
adagrad_HSS = np.zeros([features, 1]) # [HSS] Historical Sum of Grdient Square ä½¿æ¯å€‹åƒæ•¸çš„ Learning rateè®Šå¾—å®¢è£½åŒ–
eps = 0.00000000001 # /epsilon/ 1e-11, 1e-8, 1e-6
# SGD
## concat_x_y = np.concatenate((x_train_set, y_train_set), axis=1)
## random.shuffle(concat_x_y)
## x_train_set = concat_x_y[0:, 0:features]
## y_train_set = concat_x_y[0:, features:]
## del(concat_x_y)
## gc.collect()
## stop_loop = False
# Momentum
momentum = np.zeros([features, 1])
Lambda = 0.9 # Attenuation coefficientï¼Œç‚ºæ­·å²å‹•é‡çš„è¡°é€€ä¿‚æ•¸ï¼Œå€¼éœ€å°æ–¼ 1ï¼Œå¦å‰‡æœƒ monotonic incressing
# RMSProp
ema = np.zeros([features, 1]) # EMA (exponential moving averageï¼ŒæŒ‡æ•¸ç§»å‹•å¹³å‡) å¯èƒ½å–åç‚º prop æ¯”è¼ƒå¥½
Alpha = 0.85
# Adam
Beta_1 = 0.9
Beta_2 = 0.999
eps_adam = 0.0000001
momentum_adam = np.zeros([features, 1])
prop_adam = np.zeros([features, 1])
# ç´€éŒ„ Loss å€¼ï¼Œç¹ªåœ–ç”¨
loss_array = []
# ç´€éŒ„è¿­ä»£æ¬¡æ•¸
count = 0 
# åˆªå»éƒ¨åˆ†é …ç›®
# 5å°æ™‚
## L0 = np.zeros([item, 4])
## for k in range(15):
##   x_train_set[:,k*9+1:k*9+4+1] = L0
## L0 = np.zeros([len(x_validation), 4])
## for k in range(15):
##   x_validation[:,k*9+1:k*9+4+1] = L0
# åªæœ‰pm2.5+PM10
###L0 = np.zeros([item, 9])
###for k in range(6):
###  x_train_set[:,k*9+1:k*9+9+1] = L0
###for k in range(8,15):
###  x_train_set[:,k*9+1:k*9+9+1] = L0
###L0 = np.zeros([len(x_validation), 9])
###for k in range(6):
###  x_validation[:,k*9+1:k*9+9+1] = L0
###for k in range(8,15):
###  x_validation[:,k*9+1:k*9+9+1] = L0
# å¤šè€ƒæ…® PM2.5+? 3 6 7
L0 = np.zeros([item, 9])
for k in range(3):
  x_train_set[:,k*9+1:k*9+9+1] = L0
for k in range(4,6):
  x_train_set[:,k*9+1:k*9+9+1] = L0
for k in range(8,15):
  x_train_set[:,k*9+1:k*9+9+1] = L0
#for k in range(12,15):
#  x_train_set[:,k*9+1:k*9+9+1] = L0

# è¿­ä»£
for t in range(iter_time):
  count += 1
  # Vanilla Gradient descenting
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## w = w - learning_rate * gradient    

  # Momentum
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## momentum = Lambda * momentum - learning_rate * gradient
  ## w = w + momentum

  # AdaGrad Method [Adaptive learning rate]
  gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  adagrad_HSS += gradient ** 2
  w = w - learning_rate / np.sqrt(adagrad_HSS + eps) * gradient

  # RMSProp (Root-Mean-Square propagation) [Adaptive learning rate]
  # EMA å’Œ Momentum æœ‰é»é¡ä¼¼ï¼Œéƒ½æ˜¯ç”¨è¿­ä»£å°æ•¸ä¿‚æ•¸é”åˆ°ã€Œæ­·å²æ•¸æ“šå½±éŸ¿åŠ›æŒ‡æ•¸éæ¸›ã€
  # propagation å‚³æ’­ï¼Œå°±æ˜¯æŒ‡éš¨è‘—æ™‚é–“è¶Šé•·ã€å‚³æ’­çš„è¶Šé ï¼Œgradient**2 çš„å½±éŸ¿åŠ›è¦è¶Šå°
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## ema = Alpha * ema + (1-Alpha) * (gradient**2) # Tip:adagrad_HSS += gradient**2
  ## w = w - learning_rate / np.sqrt(ema) * gradient
  
  # Adam (Ada + momentum) SGDM + RMSProp ç¼ºé»ï¼šçœŸçš„ä¸å¤ªæœƒæ”¶æ–‚ï¼Œæœ€å¾Œä¸€ç›´éœ‡ç›ª
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## # æ­¤æ™‚çš„ momentum ç›´æ¥çµåˆ EMA çš„æ¦‚å¿µï¼Œå…©è€…æœç„¶å¾ˆåƒ
  ## momentum_adam = Beta_1 * momentum_adam + (1-Beta_1) * gradient 
  ## # ema_adam ç”¨ä¾†è‡ªå‹•èª¿æ•´ Learning rate æ‰€ä»¥è¦ç”¨é™¤çš„
  ## prop_adam = Beta_2 * prop_adam + (1-Beta_2) * (gradient**2)
  ## # de-biasing: ç”±æ–¼ Beta_1 è·Ÿ Beta_2 çš„å€¼éƒ½æ˜¯ 0.9 (æ¥è¿‘1)
  ## # å°è‡´è¿­ä»£å‰›é–‹å§‹æ™‚ï¼Œä¿‚æ•¸ (1-Beta) å€¼ç›´æ¥å½±éŸ¿ gradient çš„æ•¸å€¼ä¸å¤ å¤§ï¼Œ
  ## # å› æ­¤è¦éš¨è‘—æ™‚é–“é™¤ä¸Š (1-Beta) ä»¥ç¶­æŒç•¶ä¸‹ gradient çš„å½±éŸ¿åŠ›ä¸æœƒæ¸›å°‘
  ## momentum_hat = momentum_adam/(1-(Beta_1)**count)
  ## prop_hat = prop_adam/(1-(Beta_2)**count)
  ## w = w - (learning_rate/np.sqrt(prop_hat)+eps_adam) * momentum_hat

  # è¨ˆç®— Loss å€¼
  #loss_sse = np.sum(np.power(y_train_set - np.dot(x_train_set, w), 2)) # SSE (Sum of squared errors)
  #loss = np.sqrt(loss_sse/item) # RMSE (Root-mean-square error)
  Vali_Ave_Err = np.sqrt(np.sum((y_validation - np.dot(x_validation, w))**2)/len(y_validation))
  loss_sse = np.sum((y_validation - np.dot(x_validation, w))**2)
  loss = np.sqrt(loss_sse/len(y_validation))
  loss_array.append(loss)
  # æ–‡å­—é¡¯ç¤º Loss è®ŠåŒ–
  if (count%100==1) | (count==iter_time-1):
     print('Iter_time = ', count, "validation Loss = ", loss)
  #---------------#
  # SGD
  ## if not(stop_loop):
  ##   for n in range(item):
  ##     count += 1
  ##     if(count>150000):
  ##       count -= 1
  ##       stop_loop = True
  ##       break
  ##     x_n = x_train_set[n,:].reshape(1, features)
  ##     y_n = y_train_set[n].reshape(1, 1)
  ##     gradient = (-2) * np.dot(x_n.transpose(), (y_n - np.dot(x_n, w))) # features*1
  ##     w = w - learning_rate * gradient 
  ##     ## adagrad_HSS += gradient ** 2
  ##     ## w = w - learning_rate/np.sqrt(adagrad_HSS + eps) * gradient
  ##     loss_sse = np.sum(np.power(y_train_set - np.dot(x_train_set, w), 2)) # SSE (Sum of squared errors)
  ##     loss = np.sqrt(loss_sse/item) # RMSE (Root-mean-square error)
  ##     loss_array.append(loss)
  ##     if (not(count%10000)):
  ##        print('Iter_time = ', count, "Loss(error) = ", loss)
  ## else:
  ##   break
  # BGD (Batch GD)
# å°‡é‡è¦çš„å‡½æ•¸æ¬Šé‡å€¼å­˜æª”
np.save(r'LinearRegression\TrainingData_2019_Pinzhen\weight.npy', w)
with open(r'LinearRegression\PredictionResult\Iter_loss.csv', mode='w', newline='') as file:
    csv_writer = csv.writer(file)
    header = ['Iter', 'loss value']
    # print(header)
    csv_writer.writerow(header)
    for i in range(len(loss_array)):
      if(i%100==0)|(i==len(loss_array)-1):
        row = [str(i), loss_array[i]]
        csv_writer.writerow(row)
'''
[Training]
1. å‰µé€ Linear Model: weight, bias
2. è£½ä½œLoss Functionä¾†è¡¡é‡Functionçš„é æ¸¬æº–åº¦
3. ä½¿ç”¨Gradient descentè¨ˆç®—weight, biaså°Loss Functionçš„å¾®åˆ†ï¼Œä»¥æ­¤æ–¹å¼æœ‰æ•ˆç‡åœ°èª¿æ•´åƒæ•¸ï¼Œ
   å¿«é€Ÿæ‰¾åˆ°æœ€å¥½çš„åƒæ•¸å€¼çµ„åˆ

ç¯„ä¾‹Codeèªªæ˜ï¼š
   ä¸‹é¢çš„ code æ¡ç”¨ Root Mean Square Error (å‡æ–¹æ ¹èª¤å·®)
   å› ç‚ºå¸¸æ•¸é …çš„å­˜åœ¨ï¼Œæ‰€ä»¥ feature_weightsension (feature_weights) éœ€è¦å¤šåŠ ä¸€æ¬„ï¼›
   eps é …æ˜¯é¿å… adagrad çš„åˆ†æ¯ç‚º 0 è€ŒåŠ çš„æ¥µå°æ•¸å€¼ã€‚
   æ¯ä¸€å€‹ feature_weightsension (feature_weights) æœƒå°æ‡‰åˆ°å„è‡ªçš„ gradient, weight (w)ï¼Œ
   é€éè¿­ä»£ (iter_time, iteration) èª¿æ•´åƒæ•¸ã€‚

# -----ä»¥ä¸‹ç­†è¨˜----- #

# å‡è¨­ Function Model
å‡è¨­é æ¸¬å€¼ç‚º Y_pï¼Œè¼¸å…¥çš„ 9å°æ™‚ * 15å€‹ Features ç‚º X_iï¼Œ
æ¯å€‹æ™‚ç©ºçš„ Feature éƒ½ç¨ç«‹ï¼Œæ•… i ç‚º 9 * 15 = 135
Y_p = bias + w1 * X1 + w2 * X2 + ... + w15 * X15
    = bias + Summation_i^135(w_i * X_i)

# Loss Function (æå¤±å‡½æ•¸) çš„è£½ä½œï¼šå¯ç”¨ä»»ä½•åˆç†çš„æ–¹å¼è©•æ–·Functioné æ¸¬æ•ˆæœçš„å„ªåŠ£
  èª²å ‚æ•™å°çš„æ–¹æ³•æ˜¯ç”¨é¡ä¼¼çµ±è¨ˆå­¸çš„ Variance (è®Šç•°æ•¸) åšè©•ä¼°
  * Variance è®Šç•°æ•¸ å°±æ˜¯æ¯ä¸€å€‹è§€å¯Ÿå€¼èˆ‡å¹³å‡æ•¸çš„çš„è·é›¢å¹³æ–¹å’Œçš„å¹³å‡
  * æ­¤å°‡ Variance çš„å¹³å‡é›¢å‡å·®ç‰¹æ€§è¦–ç‚ºã€Œä¼°è¨ˆèª¤å·®ã€ï¼Œè¼¸å…¥è³‡æ–™çš„å€‹æ•¸å°±æ˜¯ Training datas çš„ç¸½æ•¸ï¼Œ
    ç‰¹åˆ¥ä¹‹è™•åœ¨æ–¼é€™è£¡ Variance çš„ã€Œå‡æ•¸ã€ä¹Ÿå°±æ˜¯æŒ‡ä¼°è¨ˆèª¤å·®çš„Labelå€¼æ˜¯è¦éš¨è‘—ä¸åŒçµ„ Training data è€Œ
    è®Šå‹•çš„ï¼Œä¸¦éå‚³çµ±é›¢å‡å·®æœ‰ä¸€å€‹å›ºå®šçš„å‡å€¼ã€‚
  * ç¸½çµæ­¤ Loss Function è¼¸å…¥ (w, b) å³ Function çš„ä¸€çµ„ä¿‚æ•¸ã€Training datas çš„ Input åŠ Outputï¼Œ
    è¼¸å‡ºçš„ Loss å€¼ç‚ºã€ŒLabeläº‹å¯¦å’Œé æ¸¬çµæœä¹‹é–“çš„å‡æ–¹å·®ã€
  * ç›®æ¨™æ˜¯èª¿æ•´è¼¸å…¥çš„åƒæ•¸ (w, b) ï¼Œæ‰¾åˆ°å¤ å°çš„ Loss å€¼ï¼Œå°±æ˜¯æœ€ä½³ Functionåƒæ•¸ï¼Œ
    èª¿æ•´åƒæ•¸é€™ä»¶äº‹æƒ…ä¸ç”¨æ‰‹å‹•èª¿ï¼Œä¹Ÿä¸æ˜¯æš´åŠ›æ³•ç›´æ¥é–‹ï¼Œæ­¤é¡Œå¯ä»¥ç”¨ç·šæ€§ä»£æ•¸è§£å‡º Close Formï¼Œ
    ä¹Ÿèƒ½ä½¿ç”¨ Gradient descent ä¾†åšã€‚

# Gradient descent (æ¢¯åº¦ä¸‹é™æ³•)
  * ç›®çš„ï¼šç‚º Loss Function æ‰¾åˆ°æ¯”è¼ƒå¥½çš„åƒæ•¸ Wï¼Œä½¿ Loss(W) å€¼æœ€å°ã€‚
  * é™åˆ¶ï¼šæ‰€å®šç¾©çš„ Loss Function å¿…éœ€å¯å¾®åˆ†
  * å¯¦ä½œï¼šéš¨æ©Ÿä»¤ W åˆå€¼ï¼Œè¨ˆç®— W å° Loss Function çš„å¾®åˆ†å€¼ gdï¼Œ
    è‹¥ gd æ˜¯æ­£æ•¸ (æ–œç‡ç‚ºæ­£) ä¸‹ä¸€è¼ªå°±æ¸›å°‘ Wï¼Œåä¹‹ gd ç‚ºè² æ•¸ï¼Œå‰‡åœ¨ä¸‹ä¸€è¼ªå¢åŠ  W çš„å€¼ã€‚
  * å…¬å¼ï¼šè³¦äºˆä¸€å€‹ weight å€¼ (ç¨±ç‚ºLearning rateï¼Œä»£è™Ÿæ˜¯ Î· /Eta/) ä¾†èª¿æ•´ä¸‹ä¸€è¼ªè©²
    å¢åŠ /æ¸›å°‘ W å¤šå°‘å€¼ (ä»¥æ­¤æå‡èª¿æ•´åƒæ•¸çš„æ•ˆç‡)ã€‚ä»¤ W' ç‚ºä¸‹ä¸€è¼ªåƒæ•¸å€¼ï¼Œå…¬å¼ç‚º {W' = W - Eta * gd}
  * Code èªªæ˜ï¼š
    # è§£é‡‹æ¢¯åº¦ (gradient) çš„é‹ç®—ï¼š
    # ä»¤ è³‡æ–™çš„æ•¸é‡ç‚º item = 8,892  
    # ä»¤ features çš„æ•¸é‡ç‚º feature_weights = 135
    # ä»¤ è¨“ç·´è³‡æ–™ç‚º xï¼ŒäºŒç¶­çŸ©é™£ (item*feature_weights)
    # ä»¤ åƒæ•¸(æ¬Šé‡å€¼ï¼Œå…§æ¶µ Bias)ç‚º wï¼ŒäºŒç¶­çŸ©é™£ (feature_weights*1) 
    # ä»¤ Ground truth ç‚º yï¼ŒäºŒç¶­çŸ©é™£ (item*1)
    # ä»¤ æ¢¯åº¦ ( w å° SSE (Sum Square Error, èª¤å·®å¹³æ–¹å’Œ) è¶¨è¿‘é›¶çš„å¾®åˆ†è¨ˆç®—çµæœ) ç‚º GD = (np.dot(x, w) - y)ï¼ŒäºŒç¶­çŸ©é™£(item*1)
    # ä»¤ è½‰ç½®çŸ©é™£é‹ç®—å­ç‚º ^T
    # æ¢¯åº¦é‹ç®—çš„è½‰ç½®ç°¡åŒ–æ¨å°ï¼š2 * (çŸ©é™£GD^T * çŸ©é™£x)^Tï¼Œå¾—åˆ°ä¸€å€‹ feature_weights*1 çš„çµæœï¼Œä¾ç…§è½‰ç½®é‹ç®—åŒ–ç°¡è®Šæˆ 2*(çŸ©é™£x^T * çŸ©é™£GD)
'''
## # Real Train wirh whole training set
## import random
## feature_weights = 15 * 9 + 1 # å› ç‚ºæœ‰å¸¸æ•¸é …åƒæ•¸ biasï¼Œæ‰€ä»¥ feature_weights éœ€è¦å¤šåŠ ä¸€æ¬„
## item = len(x)
## w = np.zeros([feature_weights, 1]) # [feature_weights, 1]å’Œ(feature_weights, 1)ä¸€æ¨£æ„æ€ï¼Œå°±æ˜¯å­˜æˆ feature_weights * 1 çš„äºŒç¶­é›¶çŸ©é™£
## x = np.concatenate((np.ones([item, 1]), x), axis = 1).astype(float) # å› ç‚ºå¸¸æ•¸é …çš„å­˜åœ¨ï¼Œæ‰€ä»¥ feature_weightsension (feature_weights) éœ€è¦å¤šåŠ ä¸€æ¬„
## # ä»¥ä¸‹ç‚ºèª¿æ•´ Gradient çš„é‡è¦åƒæ•¸ 
# SavingDialog = True
## Model = 'One' #One_5hour-F12345F10F12F13
## Gradient_Method = 'Vanilla'
## learning_rate = 0.01 # gradient descent çš„å¸¸ä¿‚æ•¸ ğœ‚ /Eta/
## iter_time = 15000 # gradient descent çš„è¿­ä»£æ¬¡æ•¸
## # AdaGrad åƒæ•¸
## adagrad_HSS = np.zeros([feature_weights, 1]) # ä»£è™Ÿ ğœ^t æ„æ€æ˜¯ç¬¬ t æ¬¡è¿­ä»£ä»¥å‰çš„æ‰€æœ‰æ¢¯åº¦æ›´æ–°å€¼ä¹‹å¹³æ–¹å’Œ [HSS] Historical Sum of Grdient Square
## eps = 0.0000000001 # /epsilon/ ç”¨é€”æ˜¯é¿å…åœ¨ Local minima (å¾®åˆ†ç‚ºé›¶æ™‚) åœä¸‹ä¾†
## # RMSProp
## prop = np.zeros([feature_weights, 1]) # EMA (exponential moving averageï¼ŒæŒ‡æ•¸ç§»å‹•å¹³å‡) å¯èƒ½å–åç‚º prop æ¯”è¼ƒå¥½
## Alpha = 0.85
## # SGD
## ## concat_x_y = np.concatenate((x, y), axis=1)
## ## random.shuffle(concat_x_y)
## ## x = concat_x_y[0:, 0:feature_weights]
## ## y = concat_x_y[0:, feature_weights:]
## ## del(concat_x_y)
## ## gc.collect()
## # ç´€éŒ„ Loss å€¼ï¼Œç¹ªåœ–ç”¨
## loss_array = []
## # ç´€éŒ„è¿­ä»£æ¬¡æ•¸
## count = 0 
## 
## # [æ”¹è‰¯ï¼šç¯©å»éƒ¨åˆ† Feature]
## ## L0 = np.zeros([item, 18])
## ## x[:, 109:127] = L0 # 'WIND_DIREC', 'WIND_SPEED
## ## L0 = np.zeros([item, 45])
## ## x[:, 9:54] = L0 # 'CO', 'NO', 'NO2', 'NOx', 'O3'
## ## L0 = np.zeros([item, 9])
## ## x[:, 90:99] = L0 # SO2
## ## L0 = np.zeros([item, 4])
## ## for k in range(15):
## ##   x[:,k*9+1:k*9+4+1] = L0
## # Fullå»æ‰
## ## L0 = np.zeros([item, 9])
## ## for k in range(6):
## ##   x[:,k*9+1:k*9+9+1] = L0
## ## for k in range(8,15):
## ##   x[:,k*9+1:k*9+9+1] = L0
## 
## for t in range(iter_time):
##   count += 1
##   # Vanilla
##   gradient = (-2) * np.dot(x.transpose(), (y - np.dot(x, w))) #feature_weights*1
##   w = w - learning_rate * gradient    
## 
##   # ä½¿ç”¨ AdaGrad
##   ## gradient = (-2) * np.dot(x.transpose(), (y - np.dot(x, w))) #feature_weights*1
##   ## adagrad_HSS += gradient ** 2
##   ## w = w - learning_rate / np.sqrt(adagrad_HSS + eps) * gradient
## 
##   # RMSProp (Root-Mean-Square propagation) [Adaptive learning rate]
##   # EMA å’Œ Momentum æœ‰é»é¡ä¼¼ï¼Œéƒ½æ˜¯ç”¨è¿­ä»£å°æ•¸ä¿‚æ•¸é”åˆ°ã€Œæ­·å²æ•¸æ“šå½±éŸ¿åŠ›æŒ‡æ•¸éæ¸›ã€
##   # propagation å‚³æ’­ï¼Œå°±æ˜¯æŒ‡éš¨è‘—æ™‚é–“è¶Šé•·ã€å‚³æ’­çš„è¶Šé ï¼Œgradient**2 çš„å½±éŸ¿åŠ›è¦è¶Šå°
##   ## gradient = (-2) * np.dot(x.transpose(), (y - np.dot(x, w))) # features*1
##   ## prop = Alpha * prop +  (1-Alpha) * (gradient**2) # Tip:adagrad_HSS += gradient**2
##   ## w = w - (learning_rate*gradient) / np.sqrt(prop+eps)     
##     
##   # è¨ˆç®— Loss å€¼
##   loss_sse = np.sum(np.power(y - np.dot(x, w), 2)) # Loss Function: SSE (Sum of squared errors)
##   loss = np.sqrt(loss_sse/item) # rmse (Root-mean-square deviation) 
##   loss_array.append(loss) # ç´€éŒ„ loss å€¼ï¼Œç¹ªåœ–ç”¨
##   # æ–‡å­—é¡¯ç¤º Loss è®ŠåŒ–
##   if (not(t%1000)) | (t==iter_time-1):
##     print('Iter_time = ', t, "Loss(error) = ", loss)
##   # SGD
##   ## for n in range(item):
##   ##   count += 1
##   ##   x_n = x[n,:].reshape(1, feature_weights)
##   ##   y_n = y[n].reshape(1, 1)
##   ##   gradient = (-2) * np.dot(x_n.transpose(), (y_n - np.dot(x_n, w))) # features*1
##   ##   w = w - learning_rate * gradient  
##   ##   #adagrad_HSS += gradient ** 2
##   ##   #w = w - learning_rate/np.sqrt(adagrad_HSS + eps) * gradient
##   ##   loss_sse = np.sum(np.power(y - np.dot(x, w), 2)) # SSE (Sum of squared errors)
##   ##   loss = np.sqrt(loss_sse/item) # RMSE (Root-mean-square error)
##   ##   loss_array.append(loss)
##   ##   if (not(count%10000)):
##   ##      print('Iter_time = ', count, "Loss(error) = ", loss)
## 
## # å°‡é‡è¦çš„å‡½æ•¸æ¬Šé‡å€¼å­˜æª”
## np.save(r'LinearRegression\TrainingData_2019_Pinzhen\weight.npy', w)
'''
[Testing]
# æ¸¬è©¦è³‡æ–™ä¹Ÿè¦ç¶“éæ¨™æº–åŒ–è™•ç†æ‰èƒ½è¼¸å…¥ Function
è¼‰å…¥ test dataï¼Œä¸¦ä¸”ä»¥ç›¸ä¼¼æ–¼è¨“ç·´è³‡æ–™é å…ˆè™•ç†å’Œç‰¹å¾µèƒå–çš„æ–¹å¼è™•ç†ï¼Œ
ä½¿ test data å½¢æˆ 240 å€‹ç¶­åº¦ç‚º 15 * 9 + 1 çš„è³‡æ–™ã€‚
'''
# [load data]
## test_x = np.load(r'LinearRegression\TestingData\x_test_shuffle.npy')
## test_y = np.load(r'LinearRegression\TestingData\y_test_shuffle.npy')
test_x = np.load(r'LinearRegression\TestingData\x_test_shuffle2_publuc.npy')
test_y = np.load(r'LinearRegression\TestingData\y_test_shuffle2_publuc.npy')

# [Scaling]
# Z-Score
# std_x, mean_x è¦ä»¥è¨“ç·´è³‡æ–™çš„æ•¸å€¼é€²è¡Œæ¨™æº–åŒ–ï¼Œæ‰èƒ½å°‡æ¸¬è©¦è³‡æ–™è½‰æˆç›¸åŒçš„æ¯”ä¾‹å°ºé€²è¡Œé‹ç®— 
for i in range(len(test_x)): # äºŒç¶­é™£åˆ—çš„é•·åº¦æ˜¯ç®—æœ€å¤–æ¡†è£¡é¢å…§æ¶µçš„ä¸€ç¶­é™£åˆ—å€‹æ•¸ï¼Œå› æ­¤æ˜¯ 240ï¼Œå³è¼¸å…¥çš„æ¸¬è©¦è³‡æ–™é …ç›®å€‹æ•¸
    for j in range(len(test_x[0])): # 135 å€‹ Features
        if std_x[j] != 0: # æ ¹æ“šé™¤æ³•å®šè£¡ï¼Œåˆ†æ¯ä¸å¾—ç‚ºé›¶
            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]
# min-max
## for i in range(len(test_x)): #12 * 471
##     for j in range(len(test_x[0])): #15 * 9 
##       if test_x[i][j] - min_x < 0:
##         test_x[i][j] = (test_x[i][j] - min_x) * (-1)
##       else:
##         test_x[i][j] = (test_x[i][j] - min_x) / full_range_x
# [å¢åŠ å¸¸æ•¸ç³»æ•¸]
test_x = np.concatenate((np.ones([len(test_x), 1]), test_x), axis = 1).astype(float)
test_x
# print(test_x)
# é®ç½©ä¸€äº› Features 9:54
## L0 = np.zeros([len(test_x), 18])
## test_x[:,109:127] = L0
## L0 = np.zeros([len(test_x), 45])
## test_x[:,9:54] = L0
## L0 = np.zeros([len(test_x), 9])
## test_x[:,90:99] = L0
## L0 = np.zeros([len(test_x), 4])
## for k in range(15):
##   test_x[:,k*9+1:k*9+4+1] = L0
# åªå‰© PM2.5+PM10
###L0 = np.zeros([len(test_x), 9])
###for k in range(6):
###  test_x[:,k*9+1:k*9+9+1] = L0
###for k in range(8,15):
###  test_x[:,k*9+1:k*9+9+1] = L0

# åªå‰© PM2.5 + ???
L0 = np.zeros([len(test_x), 9])
for k in range(3):
  test_x[:,k*9+1:k*9+9+1] = L0
for k in range(4,6):
  test_x[:,k*9+1:k*9+9+1] = L0
for k in range(8,15):
  test_x[:,k*9+1:k*9+9+1] = L0
#for k in range(12,15):
#  test_x[:,k*9+1:k*9+9+1] = L0
# best 5.259926965
'''
[Prediction]
ç¾åœ¨æˆ‘å€‘å·²å®šå‡º Model (é æ¸¬æ¨¡å‹, Functuon set)ã€
æ‰¾åˆ°è‡ªèªå®Œç¾ Function çš„ä¿‚æ•¸çµ„åˆã€é¸å¥½äº†æ¸¬è©¦è³‡æ–™é›†ï¼Œ
é‚£å°±èƒ½é€²è¡Œé æ¸¬äº†~
'''
w = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\weight.npy')
ans_y = np.dot(test_x, w) # [item*feature_weights] * [feature_weights*1] = [item*1] çŸ©é™£ç›¸ä¹˜çš„å¥§ç¥•å°±æ˜¯ï¼š
                          # è¢«å·¦ä¹˜çŸ©é™£ï¼Œå‰‡åˆ—é‹ç®—ï¼›è¢«å³ä¹˜ä¸€å€‹çŸ©é™£ï¼Œå‰‡è¡Œé‹ç®—ã€‚
                          # çŸ©é™£ç›¸ä¹˜æ˜¯ç›¸å°çš„æ¦‚å¿µï¼Œå–®çœ‹ä½ çš„ä¸»é«”æ˜¯èª°ã€‚
                          # ç”¨çŸ©é™£ç›¸ä¹˜å¯è¡¨ç¤ºæ–¹ç¨‹å¼çš„ã€Œä¿‚æ•¸èˆ‡æœªçŸ¥æ•¸ç›¸ä¹˜å†ç›¸åŠ ã€ï¼Œå³ç·šæ€§çµ„åˆ
ans_y
#print(ans_y)
'''
[Save prediction result to CSV file]
'''
with open(r'LinearRegression\PredictionResult\PredictionResult.csv', mode='w', newline='') as submit_file:
    csv_writer = csv.writer(submit_file)
    header = ['item_id', 'value']
    # print(header)
    csv_writer.writerow(header)
    for i in range(len(ans_y)):
        row = ['id_' + str(i), ans_y[i][0]]
        csv_writer.writerow(row)
        # print(row)
'''
[ç´€éŒ„å¯¦é©—æ•¸æ“š] ---- Functionçš„æ”¹å–„ ----
ç”¨å„å¼ Model æ¯”è¼ƒè¼¸å…¥ validation_set é ä¼°çµæœçš„ Average Error ä¾†é¸æ“‡æ›´å¥½çš„ Model
åˆ‡å‹¿ç”¨ Testing data ä¾†åšç¯©é¸ï¼Œå¦å‰‡æœƒå°è‡´ Model é æ¸¬ Private Test data çš„çµæœè®Šå·®
[Validation/ Train] Line change: 300, 301, 328 (326, 323)
'''
# No AdaGrad, all testing data: 5.158543826472928 iter:1000 ETA:0.000001
import datetime
# é¡¯ç¤ºå¯¦é©—æ•¸æ“š
Test_Ave_Err = np.sqrt(np.sum((test_y - ans_y)**2)/len(test_y))
# Validation æª¢æ¸¬æ™‚è¨­æ­¤å€¼
Vali_Ave_Err = np.sqrt(np.sum((y_validation - np.dot(x_validation, w))**2)/len(y_validation))
# å…¨éƒ¨è¨“ç·´é›†çš„æ™‚å€™è¨­æ­¤å€¼
## y_validation = '' # Validation æª¢æ¸¬æ™‚æ­¤è¡Œè¦å±è”½
## Vali_Ave_Err = 0 # Validation æª¢æ¸¬æ™‚æ­¤è¡Œè¦å±è”½

# è¨ˆç®—è¨“ç·´è³‡æ–™çš„éŒ¯èª¤ç‡`ï¼Œå…¬å¼ï¼š çœŸå¯¦å€¼-é æ¸¬å€¼/çœŸå¯¦å€¼ * 100% 
Train_error_rate = round((np.sqrt(loss_sse)/np.sqrt(np.sum(y**2)))*100, 2)
# print('Train Ave_err: ', loss)
# print('Validation Ave_err: ', Vali_Ave_Err)
# print('Testing Ave_err: ', Test_Ave_Err)
time = (datetime.datetime.now()).strftime("%y/%m/%d %H:%M:%S")
row =   [time, Model, item, 
        len(y_validation), len(test_y), 
        Normalization_Method, learning_rate, 
        count, Gradient_Method, 
        str(Train_error_rate)+'%', loss, 
        Vali_Ave_Err, Test_Ave_Err,]
head =  ['Date', 'Model','Train set size', 
         'Validation set size','Test set size', 
         'Normalization', 'Learning rate', 
         'Iteration time', 'Gradient', 
         'Train error rate', 'Train Ave_err', 
         'Validation Ave_err', 'Testing Ave_err',]
Record = pd.DataFrame({time:row[1:]}, index=head[1:])
print('[Record]:\n', Record)

# å­˜æª”
def SaveRecord(a_head, a_row):
    with open(r'LinearRegression\PredictionResult\ExperimentRecord.csv', mode='a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        header = a_head
        #writer.writerow(header) # å…ˆç”¨ mode w å¯« headerï¼Œä¹‹å¾Œç”¨ mode a æ–°å¢æ•¸æ“š
        writer.writerow(a_row)
        print('Save record successfully!')
# SaveRecord(head, row)
'''
[Loss è®ŠåŒ–åˆ†æ]
'''
# ç¹ªè£½ã€Œå‡½æ•¸çš„ Loss å€¼æ™‚è®Šåœ–ã€
plot_len = len(loss_array)
plt.axis([0, plot_len, 0, max(loss_array)+1]) # äºŒç¶­åº§æ¨™åœ– x, y è»¸çš„é¡¯ç¤ºç¯„åœ
x_pos = np.linspace(0, plot_len, plot_len) # ä¾ç…§è¿­ä»£æ¬¡æ•¸ç”¢ç”Ÿ x è»¸åº§æ¨™
y_pos = loss_array # y è»¸æ˜¯ Loss value
plt.plot(x_pos, y_pos, '-', c='blue', markersize=4) # å½¢æˆæ¯å€‹é»ï¼Œç¹ªå‡ºå‡½æ•¸åœ–å½¢
plt.show() # é¡¯ç¤º plot è¦–çª—

'''
[å¯¦é©—ç´€éŒ„æ˜¯å¦å­˜æª”ï¼Ÿ]
'''
while SavingDialog:
  save = False
  c = input('Save record? [y/n]')
  if len(c) == 1:
    if(c == 'y')|(c == 'Y'):
      save = True
      SaveRecord(head, row)
      break
    elif (c == 'n')|(c == 'N'):
      print('unsave.')
      break
    else:
      print('plz enter again, only one character \'y\' or \'n\'.')
      continue
  else:
    print('plz enter again, only one character \'y\' or \'n\'.')
    continue