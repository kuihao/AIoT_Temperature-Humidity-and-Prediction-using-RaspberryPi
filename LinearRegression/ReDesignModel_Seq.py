'''
# é‡æ–°è¨­è¨ˆé æ¸¬æ¨¡å‹ (é æ¸¬æ¡ƒåœ’å¹³é® 2019 PM 2.5)
'''
import sys
import numpy as np 
import matplotlib.pyplot as plt #ç•«å‡ºåœ–å‹ 
import pandas as pd #è³‡æ–™è™•ç†
import csv
import gc
# pd.set_option("display.max_rows", 1000)    #è¨­å®šæœ€å¤§èƒ½é¡¯ç¤º1000rows
# pd.set_option("display.max_columns", 1000) #è¨­å®šæœ€å¤§èƒ½é¡¯ç¤º1000columns
'''
[Normalize (1)]
æ¨™æº–åŒ–ï¼šæ¸¬é‡ä¸€çµ„æ•¸å€¼çš„é›¢æ•£ç¨‹åº¦ä½¿ç”¨ mean(x) å¹³å‡å€¼ã€Standard Deviation æ¨™æº–å·®
    æ–¹æ³•1 ã€Max-Minã€‘å¸¸è¦‹çš„è³‡æ–™æ¨™æº–åŒ–æ–¹æ³•ï¼Œç°¡å–®ä¾†èªªï¼Œå°‡åŸå§‹è³‡æ–™çš„æœ€å¤§ã€æœ€å°å€¼mappingè‡³å€é–“[0,1]
          å…¬å¼ï¼šx_normalized = x - min(x) / max(x) - min(x)
    æ–¹æ³•2 ã€Z-Score (Zåˆ†æ•¸)ã€‘åˆ©ç”¨åŸå§‹è³‡æ–™çš„å‡å€¼ï¼ˆmeanï¼‰å’Œæ¨™æº–å·®ï¼ˆstandard deviationï¼‰é€²è¡Œ
          è³‡æ–™çš„æ¨™æº–åŒ–ï¼Œé©ç”¨æ–¼è³‡æ–™çš„æœ€å¤§å€¼å’Œæœ€å°å€¼æœªçŸ¥çš„æƒ…æ³ï¼Œæˆ–æœ‰è¶…å‡ºå–å€¼ç¯„åœçš„é›¢ç¾¤è³‡æ–™çš„æƒ…æ³ã€‚
          å…¬å¼ï¼šæ–°è³‡æ–™ =ï¼ˆåŸå§‹è³‡æ–™-å‡å€¼ï¼‰/ æ¨™æº–å·®
'''
Normalization_Method = 'Z-Score'
x = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\x_tran_shuffle.npy')
y = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\y_tran_shuffle.npy')
mean_x = np.mean(x, axis = 0) # 15 * 9: axis = 0 æ˜¯å–é‰›ç›´æ–¹å‘ï¼Œå°12 * 471çš„è³‡æ–™ç®— Meanï¼Œå› æ­¤æœƒæœ‰ 15 * 9 å€‹ Mean å€¼ 
std_x = np.std(x, axis = 0) # 15 * 9 
for i in range(len(x)): # 12 * 471
    for j in range(len(x[0])): # 15 * 9 
        if std_x[j] != 0:
            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]
'''
[Split Training Data]
train_set and validation_set
'''
import math
x_train_set = x[: math.floor(len(x) * 0.8), :]
y_train_set = y[: math.floor(len(y) * 0.8), :]
x_validation = x[math.floor(len(x) * 0.8): , :]
y_validation = y[math.floor(len(y) * 0.8): , :]
'''
[Partial training and Validation test]
'''
import random
features = 15 * 9 + 1
item = len(x_train_set)
w1 = np.zeros([features, 1]) # features*1 
w2 = np.zeros([features, 1]) # features*1 
x_train_set = np.concatenate((np.ones([item, 1]), x_train_set), axis=1).astype(float) # å¸¸æ•¸é …æ°´å¹³åˆä¸Š x_train_set 
x_validation = np.concatenate((np.ones([len(x_validation), 1]), x_validation), axis=1).astype(float)
# ä»¥ä¸‹ç‚ºèª¿æ•´ Gradient çš„é‡è¦åƒæ•¸ 
Model = 'Square'
SavingDialog = True
Gradient_Method = 'MBGD+Momentum'
learning_rate = 0.0000001
iter_time = 150000
# AdaGrad åƒæ•¸
adagrad_HSS = np.zeros([features, 1]) # [HSS] Historical Sum of Grdient Square ä½¿æ¯å€‹åƒæ•¸çš„ Learning rateè®Šå¾—å®¢è£½åŒ–
eps = 0.00000000001 # /epsilon/ 1e-11, 1e-8, 1e-6
# SGD
concat_x_y = np.concatenate((x_train_set, y_train_set), axis=1)
random.shuffle(concat_x_y)
x_train_set = concat_x_y[0:, 0:features]
y_train_set = concat_x_y[0:, features:]
del(concat_x_y)
gc.collect()
stop_loop = False
epoch = iter_time
#batch_size = item
#iter_time = math.ceil(iter_time/batch_size)
# MBGD
batch_size = int(0.1*item)
iter_time = math.ceil(iter_time/(item/batch_size))
# Momentum
momentum = np.zeros([features, 1])
momentum2 = np.zeros([features, 1])
Lambda = 0.9 # Attenuation coefficientï¼Œç‚ºæ­·å²å‹•é‡çš„è¡°é€€ä¿‚æ•¸ï¼Œå€¼éœ€å°æ–¼ 1ï¼Œå¦å‰‡æœƒ monotonic incressing
# RMSProp
ema = np.zeros([features, 1]) # EMA (exponential moving averageï¼ŒæŒ‡æ•¸ç§»å‹•å¹³å‡) å¯èƒ½å–åç‚º prop æ¯”è¼ƒå¥½
Alpha = 0.85
# Adam
Beta_1 = 0.9
Beta_2 = 0.999
eps_adam = 0.00000001
momentum_adam = np.zeros([features, 1])
prop_adam = np.zeros([features, 1])
# ç´€éŒ„ Loss å€¼ï¼Œç¹ªåœ–ç”¨
loss_array = []
# ç´€éŒ„è¿­ä»£æ¬¡æ•¸
count = 0 
for t in range(iter_time):
  ## count += 1
  # Vanilla Gradient descenting
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## w = w - learning_rate * gradient    

  # Momentum
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## momentum = Lambda * momentum - learning_rate * gradient
  ## w = w + momentum

  # AdaGrad Method [Adaptive learning rate]
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## adagrad_HSS += gradient ** 2
  ## w = w - learning_rate / np.sqrt(adagrad_HSS + eps) * gradient

  # RMSProp (Root-Mean-Square propagation) [Adaptive learning rate]
  # EMA å’Œ Momentum æœ‰é»é¡ä¼¼ï¼Œéƒ½æ˜¯ç”¨è¿­ä»£å°æ•¸ä¿‚æ•¸é”åˆ°ã€Œæ­·å²æ•¸æ“šå½±éŸ¿åŠ›æŒ‡æ•¸éæ¸›ã€
  # propagation å‚³æ’­ï¼Œå°±æ˜¯æŒ‡éš¨è‘—æ™‚é–“è¶Šé•·ã€å‚³æ’­çš„è¶Šé ï¼Œgradient**2 çš„å½±éŸ¿åŠ›è¦è¶Šå°
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## ema = Alpha * ema + (1-Alpha) * (gradient**2) # Tip:adagrad_HSS += gradient**2
  ## w = w - learning_rate / np.sqrt(ema) * gradient
  
  # Adam (Ada + momentum) SGDM + RMSProp ç¼ºé»ï¼šçœŸçš„ä¸å¤ªæœƒæ”¶æ–‚ï¼Œæœ€å¾Œä¸€ç›´éœ‡ç›ª
  ## gradient = (-2) * np.dot(x_train_set.transpose(), (y_train_set - np.dot(x_train_set, w))) # features*1
  ## # æ­¤æ™‚çš„ momentum ç›´æ¥çµåˆ EMA çš„æ¦‚å¿µï¼Œå…©è€…æœç„¶å¾ˆåƒ
  ## momentum_adam = Beta_1 * momentum_adam + (1-Beta_1) * gradient 
  ## # ema_adam ç”¨ä¾†è‡ªå‹•èª¿æ•´ Learning rate æ‰€ä»¥è¦ç”¨é™¤çš„
  ## prop_adam = Beta_2 * prop_adam + (1-Beta_2) * (gradient**2)
  ## # de-biasing: ç”±æ–¼ Beta_1 è·Ÿ Beta_2 çš„å€¼éƒ½æ˜¯ 0.9 (æ¥è¿‘1)
  ## # å°è‡´è¿­ä»£å‰›é–‹å§‹æ™‚ï¼Œä¿‚æ•¸ (1-Beta) å€¼ç›´æ¥å½±éŸ¿ gradient çš„æ•¸å€¼ä¸å¤ å¤§ï¼Œ
  ## # å› æ­¤è¦éš¨è‘—æ™‚é–“é™¤ä¸Š (1-Beta) ä»¥ç¶­æŒç•¶ä¸‹ gradient çš„å½±éŸ¿åŠ›ä¸æœƒæ¸›å°‘
  ## momentum_hat = momentum_adam/(1-(Beta_1)**count)
  ## prop_hat = prop_adam/(1-(Beta_2)**count)
  ## w = w - (learning_rate/np.sqrt(prop_hat)+eps_adam) * momentum_hat

  # è¨ˆç®— Loss å€¼
  ## loss_sse = np.sum(np.power(y_train_set - (np.dot(x_train_set, w1) + np.dot(x_train_set**2, w2)), 2)) # SSE (Sum of squared errors)
  ## loss = np.sqrt(loss_sse/item) # RMSE (Root-mean-square error)
  ## loss_array.append(loss)
  ## # æ–‡å­—é¡¯ç¤º Loss è®ŠåŒ–
  ## if (not(count%1000)) | (count==iter_time-1):
  ##    print('Iter_time = ', count, "Loss(error) = ", loss)
  #---------------#
  # SGD
  ## if not(stop_loop):
  ##   for n in range(item):
  ##     count += 1
  ##     if(count>epoch):
  ##       count -= 1
  ##       stop_loop = True
  ##       break
  ##     x_n = x_train_set[n,:].reshape(1, features)
  ##     y_n = y_train_set[n].reshape(1, 1)
  ##     gradient1 = 2 * np.dot(x_n.transpose(), (np.dot(x_n, w1)+np.dot(x_n**2, w2) - y_n)) # features*1
  ##     gradient2 = 2 * np.dot((x_n**2).transpose(), (np.dot(x_n, w1)+np.dot(x_n**2, w2) - y_n)) # features*1
  ##     momentum = Lambda * momentum - learning_rate * gradient1
  ##     momentum2 = Lambda * momentum2 - learning_rate * gradient2
  ##     w1 = w1 + momentum
  ##     w2 = w2 + momentum2 
  ##     ## adagrad_HSS += gradient ** 2
  ##     ## w = w - learning_rate/np.sqrt(adagrad_HSS + eps) * gradient
  ##     loss_sse = np.sum(np.power(y_train_set - (np.dot(x_train_set, w1) + np.dot(x_train_set**2, w2)), 2)) # SSE (Sum of squared errors)
  ##     loss = np.sqrt(loss_sse/item) # RMSE (Root-mean-square error)
  ##     loss_array.append(loss)
  ##     if (not(count%10000)):
  ##        print('Iter_time = ', count, "Loss(error) = ", loss)
  ## else:
  ##   break
  # MBGD (Mini-Batch GD)
  if not(stop_loop):
    for n in range(math.ceil(item/batch_size)):
      count += 1
      if(batch_size*(n+1)>item):
        r = item - batch_size*n
        x_n = x_train_set[batch_size*n:batch_size*n+r,:]
        y_n = y_train_set[batch_size*n:batch_size*(n)+r]
      else:
        x_n = x_train_set[batch_size*n:batch_size*(n+1),:]
        y_n = y_train_set[batch_size*n:batch_size*(n+1)]
      gradient1 = 2 * np.dot(x_n.transpose(), (np.dot(x_n, w1)+np.dot(x_n**2, w2) - y_n)) # features*1
      gradient2 = 2 * np.dot((x_n**2).transpose(), (np.dot(x_n, w1)+np.dot(x_n**2, w2) - y_n)) # features*1
      momentum = Lambda * momentum - learning_rate * gradient1
      momentum2 = Lambda * momentum2 - learning_rate * gradient2
      w1 = w1 + momentum
      w2 = w2 + momentum2 
      ## adagrad_HSS += gradient ** 2
      ## w = w - learning_rate/np.sqrt(adagrad_HSS + eps) * gradient
      loss_sse = np.sum(np.power(y_train_set - (np.dot(x_train_set, w1) + np.dot(x_train_set**2, w2)), 2)) # SSE (Sum of squared errors)
      loss = np.sqrt(loss_sse/item) # RMSE (Root-mean-square error)
      loss_array.append(loss)
      if (not(count%10000)):
        print('Iter_time = ', count, "Loss(error) = ", loss)
      if(count>epoch):
        stop_loop = True
        break
  else:
    break
  

# å°‡é‡è¦çš„å‡½æ•¸æ¬Šé‡å€¼å­˜æª”
np.save(r'LinearRegression\TrainingData_2019_Pinzhen\weight_1.npy', w1)
np.save(r'LinearRegression\TrainingData_2019_Pinzhen\weight_2.npy', w2)
'''
[Whole Training]
'''
## # Real Train wirh whole training set
## import random
## SavingDialog = False
## feature_weights = 15 * 9 + 1 # å› ç‚ºæœ‰å¸¸æ•¸é …åƒæ•¸ biasï¼Œæ‰€ä»¥ feature_weights éœ€è¦å¤šåŠ ä¸€æ¬„
## item = len(x)
## w = np.zeros([feature_weights, 1]) # [feature_weights, 1]å’Œ(feature_weights, 1)ä¸€æ¨£æ„æ€ï¼Œå°±æ˜¯å­˜æˆ feature_weights * 1 çš„äºŒç¶­é›¶çŸ©é™£
## w1 = np.zeros([features, 1]) # features*1 
## w2 = np.zeros([features, 1]) # features*1 
## x = np.concatenate((np.ones([item, 1]), x), axis = 1).astype(float) # å› ç‚ºå¸¸æ•¸é …çš„å­˜åœ¨ï¼Œæ‰€ä»¥ feature_weightsension (feature_weights) éœ€è¦å¤šåŠ ä¸€æ¬„
## # ä»¥ä¸‹ç‚ºèª¿æ•´ Gradient çš„é‡è¦åƒæ•¸ 
## Gradient_Method = 'SGD'
## learning_rate = 0.0001 # gradient descent çš„å¸¸ä¿‚æ•¸ ğœ‚ /Eta/
## iter_time = 30 # gradient descent çš„è¿­ä»£æ¬¡æ•¸
## # AdaGrad åƒæ•¸
## adagrad_HSS = np.zeros([feature_weights, 1]) # ä»£è™Ÿ ğœ^t æ„æ€æ˜¯ç¬¬ t æ¬¡è¿­ä»£ä»¥å‰çš„æ‰€æœ‰æ¢¯åº¦æ›´æ–°å€¼ä¹‹å¹³æ–¹å’Œ [HSS] Historical Sum of Grdient Square
## eps = 0.0000000001 # /epsilon/ ç”¨é€”æ˜¯é¿å…åœ¨ Local minima (å¾®åˆ†ç‚ºé›¶æ™‚) åœä¸‹ä¾†
## # SGD
## concat_x_y = np.concatenate((x, y), axis=1)
## random.shuffle(concat_x_y)
## x = concat_x_y[0:, 0:feature_weights]
## y = concat_x_y[0:, feature_weights:]
## del(concat_x_y)
## gc.collect()
## # ç´€éŒ„ Loss å€¼ï¼Œç¹ªåœ–ç”¨
## loss_array = []
## # ç´€éŒ„è¿­ä»£æ¬¡æ•¸
## count = 0 
## for t in range(iter_time):
##   ## # Vanilla
##   ## gradient = (-2) * np.dot(x.transpose(), (y - np.dot(x, w))) #feature_weights*1
##   ## w = w - learning_rate * gradient    
## 
##   ## # ä½¿ç”¨ AdaGrad
##   ## adagrad_HSS += gradient ** 2
##   ## w = w - learning_rate / np.sqrt(adagrad_HSS + eps) * gradient    
##     
##   # SGD
##   for n in range(item):
##     count += 1
##     x_n = x[n,:].reshape(1, feature_weights)
##     y_n = y[n].reshape(1, 1)
##     gradient = (-2) * np.dot(x_n.transpose(), (y_n - np.dot(x_n, w))) # features*1
##     w = w - learning_rate * gradient  
##     #adagrad_HSS += gradient ** 2
##     #w = w - learning_rate/np.sqrt(adagrad_HSS + eps) * gradient
##     loss_sse = np.sum(np.power(y - np.dot(x, w), 2)) # SSE (Sum of squared errors)
##     loss = np.sqrt(loss_sse/item) # RMSE (Root-mean-square error)
##     loss_array.append(loss)
##     if (not(count%10000)):
##        print('Iter_time = ', count, "Loss(error) = ", loss)
## 
##     ## # è¨ˆç®— Loss å€¼
##     ##  loss_sse = np.sum(np.power(y - np.dot(x, w), 2)) # Loss Function: SSE (Sum of squared errors)
##     ##  loss = np.sqrt(loss_sse/item) # rmse (Root-mean-square deviation) 
##     ##  loss_array.append(loss) # ç´€éŒ„ loss å€¼ï¼Œç¹ªåœ–ç”¨
##     ## # æ–‡å­—é¡¯ç¤º Loss è®ŠåŒ–
##     ##  if (not(t%100)) | (t==iter_time-1):
##     ##     print('Iter_time = ', t, "Loss(error) = ", loss)
## # å°‡é‡è¦çš„å‡½æ•¸æ¬Šé‡å€¼å­˜æª”
## np.save(r'LinearRegression\TrainingData_2019_Pinzhen\weight_1.npy', w1)
## np.save(r'LinearRegression\TrainingData_2019_Pinzhen\weight_2.npy', w2)
'''
[Testing] åŒ¯å…¥æ¸¬è©¦è³‡æ–™
'''
# [load data]
## test_x = np.load(r'LinearRegression\TestingData\x_test_shuffle.npy')
## test_y = np.load(r'LinearRegression\TestingData\y_test_shuffle.npy')
test_x = np.load(r'LinearRegression\TestingData\x_test_shuffle2_publuc.npy')
test_y = np.load(r'LinearRegression\TestingData\y_test_shuffle2_publuc.npy')

# [Scaling]
# std_x, mean_x è¦ä»¥è¨“ç·´è³‡æ–™çš„æ•¸å€¼é€²è¡Œæ¨™æº–åŒ–ï¼Œæ‰èƒ½å°‡æ¸¬è©¦è³‡æ–™è½‰æˆç›¸åŒçš„æ¯”ä¾‹å°ºé€²è¡Œé‹ç®— 
for i in range(len(test_x)): # äºŒç¶­é™£åˆ—çš„é•·åº¦æ˜¯ç®—æœ€å¤–æ¡†è£¡é¢å…§æ¶µçš„ä¸€ç¶­é™£åˆ—å€‹æ•¸ï¼Œå› æ­¤æ˜¯ 240ï¼Œå³è¼¸å…¥çš„æ¸¬è©¦è³‡æ–™é …ç›®å€‹æ•¸
    for j in range(len(test_x[0])): # 135 å€‹ Features
        if std_x[j] != 0: # æ ¹æ“šé™¤æ³•å®šè£¡ï¼Œåˆ†æ¯ä¸å¾—ç‚ºé›¶
            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]
test_x = np.concatenate((np.ones([len(test_x), 1]), test_x), axis = 1).astype(float)
test_x
# print(test_x)
'''
[Prediction] å° Function è¼¸å…¥æ¸¬è©¦è³‡æ–™åŠåƒæ•¸
'''
w1 = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\weight_1.npy')
w2 = np.load(r'LinearRegression\TrainingData_2019_Pinzhen\weight_2.npy')
ans_y = np.dot(test_x, w1) + np.dot((test_x)**2, w2)
ans_y
#print(ans_y)
'''
[Save prediction result to CSV file]
'''
with open(r'LinearRegression\PredictionResult\PredictionResult.csv', mode='w', newline='') as submit_file:
    csv_writer = csv.writer(submit_file)
    header = ['item_id', 'value']
    csv_writer.writerow(header)
    for i in range(len(ans_y)):
        row = ['id_' + str(i), ans_y[i][0]]
        csv_writer.writerow(row)
        # print(row)
'''
[ç´€éŒ„å¯¦é©—æ•¸æ“š] Functionçš„æ”¹å–„
'''
import datetime
# é¡¯ç¤ºå¯¦é©—æ•¸æ“š
Vali_Ave_Err = np.sqrt(np.sum((y_validation - (np.dot(x_validation, w1) + np.dot(x_validation**2, w2)))**2)/len(y_validation))
Test_Ave_Err = np.sqrt(np.sum((test_y - ans_y)**2)/len(test_y))
## # å…¨éƒ¨è¨“ç·´é›†æ™‚ä»¥ä¸‹è§£é™¤ï¼ŒValidation æª¢æ¸¬æ™‚ä»¥ä¸‹è¦å±è”½
## y_validation = '' 
## Vali_Ave_Err = 0 
time = (datetime.datetime.now()).strftime("%y/%m/%d %H:%M:%S")
Train_error_rate = round(np.sqrt((loss_sse)/(np.sum(y**2)))*100, 2)
row =   [time, Model, item, 
        len(y_validation), len(test_y), 
        Normalization_Method, learning_rate, 
        count, Gradient_Method, 
        str(Train_error_rate)+'%', loss, 
        Vali_Ave_Err, Test_Ave_Err,]
head =  ['Date', 'Model','Train set size', 
         'Validation set size','Test set size', 
         'Normalization', 'Learning rate', 
         'Iteration time', 'Gradient', 
         'Train error rate', 'Train Ave_err', 
         'Validation Ave_err', 'Testing Ave_err',]
Record = pd.DataFrame({time:row[1:]}, index=head[1:])
print('[Record]:\n', Record)
'''
[Loss è®ŠåŒ–åˆ†æ] ç¹ªè£½ã€Œå‡½æ•¸çš„ Loss å€¼æ™‚è®Šåœ–ã€
'''
plot_len = len(loss_array)
plt.axis([0, plot_len, 0, max(loss_array)+1]) # äºŒç¶­åº§æ¨™åœ– x, y è»¸çš„é¡¯ç¤ºç¯„åœ
x_pos = np.linspace(0, plot_len, plot_len) # ä¾ç…§è¿­ä»£æ¬¡æ•¸ç”¢ç”Ÿ x è»¸åº§æ¨™
y_pos = loss_array # y è»¸æ˜¯ Loss value
plt.plot(x_pos, y_pos, '-', c='blue', markersize=4) # å½¢æˆæ¯å€‹é»ï¼Œç¹ªå‡ºå‡½æ•¸åœ–å½¢
plt.show() # é¡¯ç¤º plot è¦–çª—
'''
[å¯¦é©—ç´€éŒ„æ˜¯å¦å­˜æª”ï¼Ÿ]
'''
def SaveRecord(run_dialog ,a_head, a_row):
  while run_dialog:
    c = input('Save record? [y/n]')
    if len(c) == 1:
      if(c == 'y')|(c == 'Y'):
        print('Saving...')
        with open(r'LinearRegression\PredictionResult\ExperimentRecord_Redesign.csv', mode='a', newline='') as csvfile:
          writer = csv.writer(csvfile)
          header = a_head
          # writer.writerow(header) # å…ˆç”¨ mode w å¯« headerï¼Œä¹‹å¾Œç”¨ mode a æ–°å¢æ•¸æ“š
          writer.writerow(a_row)
          print('Save record successfully!')
        break
      elif (c == 'n')|(c == 'N'):
        print('Unsave.')
        break
      else:
        print('plz enter again, only one character \'y\' or \'n\'.')
        continue
    else:
      print('plz enter again, only one character \'y\' or \'n\'.')
      continue
SaveRecord(SavingDialog, head, row)

    
